{"pages":[{"title":"gallery","text":"","link":"/gallery/index.html"},{"title":"About Me","text":"Education 2023.9 - Current: School of Information, Renmin University of China 2019.9 - 2023.6 (BEng): Computer Science and Technology, Shandong University","link":"/about/index.html"}],"posts":[{"title":"Less is More：De-amplifying I&#x2F;Os for Key-value Stores with a Log-assisted LSM-tree","text":"We present a novel scheme, called Log-assisted LSM-tree (L2SM), which adopts a small-size, multi-level log structure to isolate selected key-value items that have a disruptive effect on the tree structure, accumulates and absorbs the repeated updates in a highly efficient manner, and removes obsolete and deleted key-value items at an early stage.","link":"/2022/03/11/Embedded/ICDE21-huang/"},{"title":"DFS Techniques","text":"Introduce two key techniques of distributed file system. Speech 下面我来给大家介绍分布式文件系统中另外两个核心技术。 首先是Scalability&amp;Usability也就是DFS的可扩展性和可用性。 高可用的metadata DFS的metadata主要包括文件的命名空间、每个文件不同副本的位置、副本的的版本号等等。 在DFS中，metadata的存储主要有两种方式，一种是集中存储，把所有的元数据都存在一个metadata server，统一管理所有的元数据；另一种是将metadata分布到多个节点进行存储。这两种方式相比，集中存储更常用，将元数据和数据分离，整个系统拥有较高的吞吐量，便于实现。 以GFS为例，GFS的metadata主要包括三部分内容：命名空间、文件到chunk的映射关系、chunk的位置。 元数据存在唯一的GFS master中，文件按chunk进行划分，每个chunk的大小为64MB，每一个chunk会在多个chunk server中保存副本（默认为3个），chunk server将chunk作为Linux file保存在本地磁盘上。 如何根据metadata进行读操作： 应用程序调用GFS client提供的接口，表明要读取的文件名、偏移、长度。 GFS Client将偏移按照规则翻译成chunk序号，发送给master。 master将chunk id与chunk的副本位置告诉GFS client。 GFS client向最近的持有副本的Chunkserver发出读请求，请求中包含chunk id与范围。 ChunkServer读取相应的文件，然后将文件内容发给GFS client。 Namespace delegation DFS的命名空间主要是指DFS对文件目录的统一管理。分布式文件系统中，需要考虑并发在同一个目录下创建文件的情况。为了防止冲突，使用锁机制保证对命名空间的互斥访问。 锁分为读锁和写锁，分别对应读操作和写操作。 e.g. 如果对 /d1/d2/…/dn/leaf 进行操作 需要获得 /d1, /d1/d2, /d1/d2/…/dn 的读锁 需要 /d1/d2/…/dn/leaf 的读锁或者写锁 通过命名空间锁可以允许在相同目录发生并发的变化。比如多个文件在同一个目录被并发创建，每个创建会申请此目录的读锁和各自文件的写锁，不会导致冲突。目录的读锁可以保护在创建时此目录不会被删除、重命名或者执行快照。对相同文件的创建请求，由于写锁的保护，也只会导致此文件被串行的创建两次。因为命名空间的节点不少，全量分配读写锁有点浪费资源，所以它们都是lazy分配、用完即删。而且锁申请不是随意的，为了防止死锁，一个操作必须按特定的顺序来申请锁：首先按命名空间树的层级排序，在相同层级再按字典序。 可扩展性 DFS有很强的可扩展性，需要注意的问题： 如何控制不同server之间的负载均衡 如何保证新加入的节点不会因短期负载压力过大而崩溃 如何更新元数据 然后是Fault-tolerance也就是容错性，DFS通过多副本机制保证容错性，副本之间要保证一致性。 Checkpointing—metadata的崩溃一致性：metadata存在master的内存中，operation log记录重要的元数据变化的历史信息，是metadata的持久化记录，我们将它重复存在多个远程机器上，直到日志记录被flush到本地磁盘以及远程机器之后才会回复客户端。 Leases—租赁机制：保证数据修改时的一致性。 由master指定primary replica和secondary replicas，60s后过期重新指定。 写操作流程： Client向master请求Chunk的副本信息，以及哪个副本（Replica）是Primary master回复client，client缓存这些信息在本地 client将数据（Data）链式推送到所有副本 Client通知Primary提交 primary在自己成功提交后，通知所有Secondary提交 Secondary向Primary回复提交结果 primary回复client提交结果 Data的一致性： 两种状态：consistent和defined，目的是在所有的replicas的执行相同的串行化操作序列保证file region的defined。 Handshake检测故障停机 Checksum检测数据可靠性 Version控制数据一致性 返回哪个副本给Client","link":"/2022/03/29/Embedded/dfs_techniques/"},{"title":"RocksDB和db_bench安装与配置","text":"Start from a new Ubuntu OS. Introduction After remaking N times, I made this blog finally. Let’s start from a new Ubuntu now. Steps Step 1 首先安装gcc、g++等工具。 1sudo apt install build-essential 然后安装一些必要的库，用于RocksDB的Compression。 1sudo apt-get install libsnappy-dev zlib1g-dev libbz2-dev liblz4-dev libzstd-dev libgflags-dev Step 2 下载RocksDB源码并解压。 12wget https://github.com/facebook/rocksdb/archive/v6.25.1.zipunzip rocksdb-6.25.1.zip Tips: 版本号可自己选择，下面涉及到版本号的命令需要对应更改。e.g. v6.6.4 (2020-01-31) 此过程需要的时间可能较长，可以通过其他方法下载zip压缩包，拷贝至Ubuntu系统。(Recommended) 如果压缩包名字略有不同，自行更改。 Step 3 编译生成动态链接库和静态链接库 123cd rocksdb-6.25.1make shared_lib &amp;&amp; sudo make install-sharedmake static_lib &amp;&amp; sudo make install-static Tips: 如果先生成静态链接库再生成动态链接库，在生成动态链接库的时候会报错。 12make static_lib &amp;&amp; sudo make install-staticmake shared_lib &amp;&amp; sudo make install-shared 解决办法如下： 123make cleanmake shared_libmake static_lib 此过程需要的时间较长（约10min）。 最后执行sudo make install命令。 1sudo make install Step 4 设置环境变量 123#echo &quot;/usr/local/lib&quot; |sudo tee /etc/ld.so.conf.d/rocksdb-x86_64.confsudo ldconfig -vmake shared_lib &amp;&amp; sudo make install-sharedsudo ldconfig -v Tips: #echo &quot;/usr/local/lib&quot; |sudo tee /etc/ld.so.conf.d/rocksdb-x86_64.confsudo ldconfig -v: refresh the ldconfig cacheINSTALL_PATH=/usr sudo ldconfig -v: refresh the ldconfig cache Test 新建测试程序rocksdbtest.cpp 12345678910111213141516171819202122232425262728293031323334353637#include &lt;cstdio&gt;#include &lt;string&gt;#include &quot;rocksdb/db.h&quot;#include &quot;rocksdb/slice.h&quot;#include &quot;rocksdb/options.h&quot;using namespace std;using namespace rocksdb;const std::string PATH = &quot;/tmp/rocksdb_tmp&quot;;int main() { DB* db; Options options; options.create_if_missing = true; Status status = DB::Open(options, PATH, &amp;db); assert(status.ok()); Slice key(&quot;foo&quot;); Slice value(&quot;bar&quot;); std::string get_value; status = db-&gt;Put(WriteOptions(), key, value); if(status.ok()) { status = db-&gt;Get(ReadOptions(), key, &amp;get_value); if(status.ok()) { printf(&quot;get %s success!!\\n&quot;, get_value.c_str()); } else { printf(&quot;get failed\\n&quot;); } } else { printf(&quot;put failed\\n&quot;); } delete db;} 动态编译 1g++ -std=c++11 -o rocksdbtest rocksdbtest.cpp -lrocksdb -lpthread 执行 1./rocksdbtest 正确结果 1get bar success!! db_bench 123make cleanmake db_bench./db_bench Tips: 运行db_bench时设置参数 e.g.1./db_bench -benchmarks=&quot;fillrandom,stats&quot; -statistics -key_size=16 -value_size=65536 -db=./test_db1 -wal_dir=./test_db1 -duration=6000 -level0_file_num_compaction_trigger=1 -enable_pipelined_write=true -compression_type=None -stats_per_interval=1 -stats_interval_seconds=10 -max_write_buffer_number=6 Reference https://blog.51cto.com/u_15081048/2592774 https://www.jianshu.com/p/575b2e27b028 https://blog.csdn.net/zhangpeterx/article/details/96869454 https://www.cxyzjd.com/article/zhangpeterx/96869454","link":"/2021/10/14/Embedded/db_bench/"},{"title":"SwapKV：A Hotness Aware In-memory Key-Value Store for Hybrid Memory Systems","text":"This paper presents SwapKV, which strives to retain both the advantages of DRAM and PMEM, aiming to achieve both high performance and large capacity simultaneously.","link":"/2022/04/16/Embedded/TKDE21-SwapKV/"},{"title":"MySQL Buffer Pool Design","text":"MySQL存储引擎InnoDB的buffer pool设计思路。 MySQL(InnoDB) buffer pool 配置参数 innodb_buffer_pool_size: buffer pool大小 innodb_buffer_pool_instances: buffer pool实例个数（若bufferpool较大，可划分为多个instances，每个instance通过各自的list独立管理，提高读并发度） innodb_buffer_pool_chunk_size: 当增加或减少innodb_buffer_pool_size时，innodb_buffer_pool_chunk_size相应变化 If the new innodb_buffer_pool_chunk_size value * innodb_buffer_pool_instances is larger than the current buffer pool size when the buffer pool is initialized, innodb_buffer_pool_chunk_size is truncated to innodb_buffer_pool_size / innodb_buffer_pool_instances. Buffer pool size must always be equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances. If you alter innodb_buffer_pool_chunk_size, innodb_buffer_pool_size is automatically adjusted to a value that is equal to or a multiple of innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances. The adjustment occurs when the buffer pool is initialized. innodb_old_blocks_pct: controls the percentage of “old” blocks in the LRU list（LRU链表中插入点的位置） 替换策略：变种LRU 普通LRU会产生的问题：预读失效和缓冲池污染。 预读失效：预先加载的一些page后续没有被访问，反而丢弃了原本LRU链表末尾的一些page。 缓冲池污染：一次性扫描大量数据，buffer pool中所有page被替换出去。 解决方案：冷热数据分离 将LRU链表分为两部分，热数据区和冷数据区。 当某一page第一次被加载到buffer pool中，先将其放到冷数据区域的链表头部。 经过innodb_old_blocks_time（单位：ms）后，若该page再次被访问，将其移动到热数据区域的链表头部。 若page已经在热数据区，再次被访问，不需要每次都移动到热数据区链表头部，MySQL的优化方案是，热数据区的后3/4部分被访问需要移动到链表头部，前1/4部分不移动。 LRU链表 分为两个部分：New Sublist，Old Sublist。 innodb_old_blocks_pct控制插入点Midpoint。 全表扫描时，设置innodb_old_blocks_time的时间窗口可以有效的保护New Sublist。 预读机制 Linear read-ahead Random read-ahead API buf0buf.h: The database buffer pool high-level routines dberr_t buf_pool_init(ulint total_size, ulint n_instances): Creates the buffer pool. void buf_pool_free_all(): Frees the buffer pool at shutdown. void buf_resize_thread(): This is the thread for resizing buffer pool. void buf_pool_clear_hash_index(void): Clears the adaptive hash index on all pages in the buffer pool. static inline ulint buf_pool_get_curr_size(void): Gets the current size of buffer buf_pool in bytes. static inline ulint buf_pool_get_n_pages(void): Gets the current size of buffer buf_pool in frames. get bool buf_page_optimistic_get(ulint rw_latch, buf_block_t *block, uint64_t modify_clock, Page_fetch fetch_mode, const char *file, ulint line, mtr_t *mtr): Get optimistic access to a database page. bool buf_page_get_known_nowait(ulint rw_latch, buf_block_t *block, Cache_hint hint, const char *file, ulint line, mtr_t *mtr): Get access to a known database page, when no waiting can be done. const buf_block_t *buf_page_try_get_func(const page_id_t &amp;page_id, const char *file, ulint line, mtr_t *mtr): Given a tablespace id and page number tries to get that page. buf_block_t *buf_page_get_gen(const page_id_t &amp;page_id, const page_size_t &amp;page_size, ulint rw_latch, buf_block_t *guess, Page_fetch mode, const char *file, ulint line, mtr_t *mtr, bool dirty_with_no_latch = false): Get access to a database page. buf_block_t *buf_page_create(const page_id_t &amp;page_id, const page_size_t &amp;page_size, rw_lock_type_t rw_latch, mtr_t *mtr): Initializes a page to the buffer buf_pool. void buf_page_make_young(buf_page_t *bpage): Moves a page to the start of the buffer pool LRU list. void buf_page_make_old(buf_page_t *bpage): Moved a page to the end of the buffer pool LRU list. static inline ibool buf_page_peek(const page_id_t &amp;page_id): Returns TRUE if the page can be found in the buffer pool hash table. buf0dblwr.h: Doublewrite buffer module buf0rea.h: The database buffer read buf0dump.h: Implements a buffer pool dump/load buf0flu.h: The database buffer pool flush algorithm buf0lru.h: The database buffer pool LRU replacement algorithm Reference MySQL源码 buffer pool API声明 buffer pool API实现 MySQL InnoDB文档","link":"/2021/09/07/Embedded/mysql_buffer_pool_design/"},{"title":"Enabling Low Tail Latency on Multicore Key-Value Stores","text":"We present RStore to enable low and predictable latency (i.e. low tail latency) and efficient use of hardware resources such as CPU, memory and storage through the following design points: Asynchronous execution Hybrid DRAM+NVM architecture Log-structured storage User-space networking","link":"/2022/04/05/Embedded/p1091-lersch/"},{"title":"RocksDB Compaction源码分析","text":"RocksDB的Compaction过程整体可分为三个部分，prepare keys、process keys、write keys。 入口：db/db_impl_compaction_flush.cc中的BackgroundCompaction() Prepare keys 触发条件 RocksDB的compaction都是后台运行，通过线程BGWorkCompaction进行compaction的调度。Compaction分为两种： Manual compaction by CompactFiles() Auto compaction by BackgroundCompaction() MaybeScheduleFlushOrCompaction 1234567891011while (bg_compaction_scheduled_ &lt; bg_job_limits.max_compactions &amp;&amp; unscheduled_compactions_ &gt; 0) { CompactionArg* ca = new CompactionArg; ca-&gt;db = this; ca-&gt;prepicked_compaction = nullptr; bg_compaction_scheduled_++; //正在被调度的compaction线程数目 unscheduled_compactions_--; //待调度的线程个数，及待调度的cfd的长度 //调度BGWorkCompaction线程 env_-&gt;Schedule(&amp;DBImpl::BGWorkCompaction, ca, Env::Priority::LOW, this, &amp;DBImpl::UnscheduleCompactionCallback);} ​ 可以看到最大线程数量限制是bg_job_limits.max_compactions。 队列DBImpl::compaction_queue_ 1std::deque&lt;ColumnFamilyData*&gt; compaction_queue_; ​ 这个队列的更新是在函数SchedulePendingCompaction更新的，且unscheduled_compactions_变量是和该函数一起更新的，也就是只有设置了该变量才能够正常调度compaction后台线程。 123456void DBImpl::SchedulePendingCompaction(ColumnFamilyData* cfd) { if (!cfd-&gt;queued_for_compaction() &amp;&amp; cfd-&gt;NeedsCompaction()) { AddToCompactionQueue(cfd); ++unscheduled_compactions_; }} ​ 上面的核心函数是NeedsCompaction,通过这个函数来判断是否有sst需要被compact。 123456789101112131415161718192021bool LevelCompactionPicker::NeedsCompaction( const VersionStorageInfo* vstorage) const { if (!vstorage-&gt;ExpiredTtlFiles().empty()) { //有超时的sst(ExpiredTtlFiles) return true; } if (!vstorage-&gt;FilesMarkedForPeriodicCompaction().empty()) { return true; } if (!vstorage-&gt;BottommostFilesMarkedForCompaction().empty()) { return true; } if (!vstorage-&gt;FilesMarkedForCompaction().empty()) { return true; } for (int i = 0; i &lt;= vstorage-&gt;MaxInputLevel(); i++) { if (vstorage-&gt;CompactionScore(i) &gt;= 1) { //遍历所有的level的sst,根据score判断是否需要compact return true; } } return false;} SST文件的选择 下面这两个变量分别保存了level以及每个level所对应的score，score越高，优先级越高。 12std::vector&lt;double&gt; compaction_score_; //当前sst的scorestd::vector&lt;int&gt; compaction_level_; //当前sst需要被compact到的层level 这两个变量的更新在函数VersionStorageInfo::ComputeCompactionScore中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125void VersionStorageInfo::ComputeCompactionScore( const ImmutableOptions&amp; immutable_options, const MutableCFOptions&amp; mutable_cf_options) { for (int level = 0; level &lt;= MaxInputLevel(); level++) { double score; if (level == 0) { // We treat level-0 specially by bounding the number of files // instead of number of bytes for two reasons: // // (1) With larger write-buffer sizes, it is nice not to do too // many level-0 compactions. // // (2) The files in level-0 are merged on every read and // therefore we wish to avoid too many files when the individual // file size is small (perhaps because of a small write-buffer // setting, or very high compression ratios, or lots of // overwrites/deletions). int num_sorted_runs = 0; uint64_t total_size = 0; for (auto* f : files_[level]) { if (!f-&gt;being_compacted) { total_size += f-&gt;compensated_file_size; num_sorted_runs++; } } if (compaction_style_ == kCompactionStyleUniversal) { // For universal compaction, we use level0 score to indicate // compaction score for the whole DB. Adding other levels as if // they are L0 files. for (int i = 1; i &lt; num_levels(); i++) { // Its possible that a subset of the files in a level may be in a // compaction, due to delete triggered compaction or trivial move. // In that case, the below check may not catch a level being // compacted as it only checks the first file. The worst that can // happen is a scheduled compaction thread will find nothing to do. if (!files_[i].empty() &amp;&amp; !files_[i][0]-&gt;being_compacted) { num_sorted_runs++; } } } if (compaction_style_ == kCompactionStyleFIFO) { score = static_cast&lt;double&gt;(total_size) / mutable_cf_options.compaction_options_fifo.max_table_files_size; if (mutable_cf_options.compaction_options_fifo.allow_compaction || mutable_cf_options.compaction_options_fifo.age_for_warm &gt; 0) { // Warm tier move can happen at any time. It's too expensive to // check very file's timestamp now. For now, just trigger it // slightly more frequently than FIFO compaction so that this // happens first. score = std::max( static_cast&lt;double&gt;(num_sorted_runs) / mutable_cf_options.level0_file_num_compaction_trigger, score); } if (mutable_cf_options.ttl &gt; 0) { score = std::max( static_cast&lt;double&gt;(GetExpiredTtlFilesCount( immutable_options, mutable_cf_options, files_[level])), score); } } else { score = static_cast&lt;double&gt;(num_sorted_runs) / mutable_cf_options.level0_file_num_compaction_trigger; if (compaction_style_ == kCompactionStyleLevel &amp;&amp; num_levels() &gt; 1) { // Level-based involves L0-&gt;L0 compactions that can lead to oversized // L0 files. Take into account size as well to avoid later giant // compactions to the base level. uint64_t l0_target_size = mutable_cf_options.max_bytes_for_level_base; if (immutable_options.level_compaction_dynamic_level_bytes &amp;&amp; level_multiplier_ != 0.0) { // Prevent L0 to Lbase fanout from growing larger than // `level_multiplier_`. This prevents us from getting stuck picking // L0 forever even when it is hurting write-amp. That could happen // in dynamic level compaction's write-burst mode where the base // level's target size can grow to be enormous. l0_target_size = std::max(l0_target_size, static_cast&lt;uint64_t&gt;(level_max_bytes_[base_level_] / level_multiplier_)); } score = std::max(score, static_cast&lt;double&gt;(total_size) / l0_target_size); } } } else { // Compute the ratio of current size to size limit. uint64_t level_bytes_no_compacting = 0; for (auto f : files_[level]) { if (!f-&gt;being_compacted) { level_bytes_no_compacting += f-&gt;compensated_file_size; } } score = static_cast&lt;double&gt;(level_bytes_no_compacting) / MaxBytesForLevel(level); } compaction_level_[level] = level; compaction_score_[level] = score; } // sort all the levels based on their score. Higher scores get listed // first. Use bubble sort because the number of entries are small. for (int i = 0; i &lt; num_levels() - 2; i++) { for (int j = i + 1; j &lt; num_levels() - 1; j++) { if (compaction_score_[i] &lt; compaction_score_[j]) { double score = compaction_score_[i]; int level = compaction_level_[i]; compaction_score_[i] = compaction_score_[j]; compaction_level_[i] = compaction_level_[j]; compaction_score_[j] = score; compaction_level_[j] = level; } } } ComputeFilesMarkedForCompaction(); ComputeBottommostFilesMarkedForCompaction(); if (mutable_cf_options.ttl &gt; 0) { ComputeExpiredTtlFiles(immutable_options, mutable_cf_options.ttl); } if (mutable_cf_options.periodic_compaction_seconds &gt; 0) { ComputeFilesMarkedForPeriodicCompaction( immutable_options, mutable_cf_options.periodic_compaction_seconds); } EstimateCompactionBytesNeeded(mutable_cf_options);} compaction每一层level大小的确定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147void VersionStorageInfo::CalculateBaseBytes(const ImmutableOptions&amp; ioptions, const MutableCFOptions&amp; options) { // Special logic to set number of sorted runs. // It is to match the previous behavior when all files are in L0. int num_l0_count = static_cast&lt;int&gt;(files_[0].size()); if (compaction_style_ == kCompactionStyleUniversal) { // For universal compaction, we use level0 score to indicate // compaction score for the whole DB. Adding other levels as if // they are L0 files. for (int i = 1; i &lt; num_levels(); i++) { if (!files_[i].empty()) { num_l0_count++; } } } set_l0_delay_trigger_count(num_l0_count); level_max_bytes_.resize(ioptions.num_levels); if (!ioptions.level_compaction_dynamic_level_bytes) { base_level_ = (ioptions.compaction_style == kCompactionStyleLevel) ? 1 : -1; // Calculate for static bytes base case for (int i = 0; i &lt; ioptions.num_levels; ++i) { if (i == 0 &amp;&amp; ioptions.compaction_style == kCompactionStyleUniversal) { level_max_bytes_[i] = options.max_bytes_for_level_base; } else if (i &gt; 1) { level_max_bytes_[i] = MultiplyCheckOverflow( MultiplyCheckOverflow(level_max_bytes_[i - 1], options.max_bytes_for_level_multiplier), options.MaxBytesMultiplerAdditional(i - 1)); } else { level_max_bytes_[i] = options.max_bytes_for_level_base; } } } else { uint64_t max_level_size = 0; int first_non_empty_level = -1; // Find size of non-L0 level of most data. // Cannot use the size of the last level because it can be empty or less // than previous levels after compaction. for (int i = 1; i &lt; num_levels_; i++) { uint64_t total_size = 0; for (const auto&amp; f : files_[i]) { total_size += f-&gt;fd.GetFileSize(); } if (total_size &gt; 0 &amp;&amp; first_non_empty_level == -1) { first_non_empty_level = i; } if (total_size &gt; max_level_size) { max_level_size = total_size; } } // Prefill every level's max bytes to disallow compaction from there. for (int i = 0; i &lt; num_levels_; i++) { level_max_bytes_[i] = std::numeric_limits&lt;uint64_t&gt;::max(); } if (max_level_size == 0) { // No data for L1 and up. L0 compacts to last level directly. // No compaction from L1+ needs to be scheduled. base_level_ = num_levels_ - 1; } else { uint64_t l0_size = 0; for (const auto&amp; f : files_[0]) { l0_size += f-&gt;fd.GetFileSize(); } uint64_t base_bytes_max = std::max(options.max_bytes_for_level_base, l0_size); uint64_t base_bytes_min = static_cast&lt;uint64_t&gt;( base_bytes_max / options.max_bytes_for_level_multiplier); // Try whether we can make last level's target size to be max_level_size uint64_t cur_level_size = max_level_size; for (int i = num_levels_ - 2; i &gt;= first_non_empty_level; i--) { //从倒数第二层level往上到first non empty level // Round up after dividing cur_level_size = static_cast&lt;uint64_t&gt;( cur_level_size / options.max_bytes_for_level_multiplier); } // Calculate base level and its size. uint64_t base_level_size; if (cur_level_size &lt;= base_bytes_min) { // Case 1. If we make target size of last level to be max_level_size, // target size of the first non-empty level would be smaller than // base_bytes_min. We set it be base_bytes_min. base_level_size = base_bytes_min + 1U; base_level_ = first_non_empty_level; ROCKS_LOG_INFO(ioptions.logger, &quot;More existing levels in DB than needed. &quot; &quot;max_bytes_for_level_multiplier may not be guaranteed.&quot;); } else { // Find base level (where L0 data is compacted to). base_level_ = first_non_empty_level; while (base_level_ &gt; 1 &amp;&amp; cur_level_size &gt; base_bytes_max) { --base_level_; cur_level_size = static_cast&lt;uint64_t&gt;( cur_level_size / options.max_bytes_for_level_multiplier); } if (cur_level_size &gt; base_bytes_max) { // Even L1 will be too large assert(base_level_ == 1); base_level_size = base_bytes_max; } else { base_level_size = cur_level_size; } } level_multiplier_ = options.max_bytes_for_level_multiplier; assert(base_level_size &gt; 0); if (l0_size &gt; base_level_size &amp;&amp; (l0_size &gt; options.max_bytes_for_level_base || static_cast&lt;int&gt;(files_[0].size() / 2) &gt;= options.level0_file_num_compaction_trigger)) { // We adjust the base level according to actual L0 size, and adjust // the level multiplier accordingly, when: // 1. the L0 size is larger than level size base, or // 2. number of L0 files reaches twice the L0-&gt;L1 compaction trigger // We don't do this otherwise to keep the LSM-tree structure stable // unless the L0 compaction is backlogged. base_level_size = l0_size; if (base_level_ == num_levels_ - 1) { level_multiplier_ = 1.0; } else { level_multiplier_ = std::pow( static_cast&lt;double&gt;(max_level_size) / static_cast&lt;double&gt;(base_level_size), 1.0 / static_cast&lt;double&gt;(num_levels_ - base_level_ - 1)); } } uint64_t level_size = base_level_size; for (int i = base_level_; i &lt; num_levels_; i++) { if (i &gt; base_level_) { level_size = MultiplyCheckOverflow(level_size, level_multiplier_); } // Don't set any level below base_bytes_max. Otherwise, the LSM can // assume an hourglass shape where L1+ sizes are smaller than L0. This // causes compaction scoring, which depends on level sizes, to favor L1+ // at the expense of L0, which may fill up and stall. level_max_bytes_[i] = std::max(level_size, base_bytes_max); } } }} static：每一层的大小都是固定的 dynamic：动态根据每一层大小进行计算 引入base level的概念，通常使用空间放大来衡量空间效率，忽略数据压缩的影响，空间放大 = size_on_file_system / size_of_user_data。 挑选参与compaction的文件 12345678910111213141516171819202122232425262728Compaction* LevelCompactionBuilder::PickCompaction() { // Pick up the first file to start compaction. It may have been extended // to a clean cut. SetupInitialFiles(); if (start_level_inputs_.empty()) { return nullptr; } assert(start_level_ &gt;= 0 &amp;&amp; output_level_ &gt;= 0); // If it is a L0 -&gt; base level compaction, we need to set up other L0 // files if needed. if (!SetupOtherL0FilesIfNeeded()) { return nullptr; } // Pick files in the output level and expand more files in the start level // if needed. if (!SetupOtherInputsIfNeeded()) { return nullptr; } // Form a compaction object containing the files we picked. Compaction* c = GetCompaction(); TEST_SYNC_POINT_CALLBACK(&quot;LevelCompactionPicker::PickCompaction:Return&quot;, c); return c;} 这里PickCompaction分别调用了三个主要的函数。 SetupInitialFiles 初始化需要compact的文件 SetupOtherL0FilesIfNeeded 如果需要的话，setup一些其他的L0文件 SetupOtherInputsIfNeeded 如果需要的话，setup一些其他的inputs 下面首先分析SetupInitialFiles。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687void LevelCompactionBuilder::SetupInitialFiles() { // Find the compactions by size on all levels. bool skipped_l0_to_base = false; for (int i = 0; i &lt; compaction_picker_-&gt;NumberLevels() - 1; i++) { start_level_score_ = vstorage_-&gt;CompactionScore(i); start_level_ = vstorage_-&gt;CompactionScoreLevel(i); assert(i == 0 || start_level_score_ &lt;= vstorage_-&gt;CompactionScore(i - 1)); if (start_level_score_ &gt;= 1) { if (skipped_l0_to_base &amp;&amp; start_level_ == vstorage_-&gt;base_level()) { // If L0-&gt;base_level compaction is pending, don't schedule further // compaction from base level. Otherwise L0-&gt;base_level compaction // may starve. continue; } output_level_ = (start_level_ == 0) ? vstorage_-&gt;base_level() : start_level_ + 1; if (PickFileToCompact()) { // found the compaction! if (start_level_ == 0) { // L0 score = `num L0 files` / `level0_file_num_compaction_trigger` compaction_reason_ = CompactionReason::kLevelL0FilesNum; } else { // L1+ score = `Level files size` / `MaxBytesForLevel` compaction_reason_ = CompactionReason::kLevelMaxLevelSize; } break; } else { // didn't find the compaction, clear the inputs start_level_inputs_.clear(); if (start_level_ == 0) { skipped_l0_to_base = true; // L0-&gt;base_level may be blocked due to ongoing L0-&gt;base_level // compactions. It may also be blocked by an ongoing compaction from // base_level downwards. // // In these cases, to reduce L0 file count and thus reduce likelihood // of write stalls, we can attempt compacting a span of files within // L0. if (PickIntraL0Compaction()) { output_level_ = 0; compaction_reason_ = CompactionReason::kLevelL0FilesNum; break; } } } } else { // Compaction scores are sorted in descending order, no further scores // will be &gt;= 1. break; } } if (!start_level_inputs_.empty()) { return; } // if we didn't find a compaction, check if there are any files marked for // compaction parent_index_ = base_index_ = -1; compaction_picker_-&gt;PickFilesMarkedForCompaction( cf_name_, vstorage_, &amp;start_level_, &amp;output_level_, &amp;start_level_inputs_); if (!start_level_inputs_.empty()) { compaction_reason_ = CompactionReason::kFilesMarkedForCompaction; return; } // Bottommost Files Compaction on deleting tombstones PickFileToCompact(vstorage_-&gt;BottommostFilesMarkedForCompaction(), false); if (!start_level_inputs_.empty()) { compaction_reason_ = CompactionReason::kBottommostFiles; return; } // TTL Compaction PickFileToCompact(vstorage_-&gt;ExpiredTtlFiles(), true); if (!start_level_inputs_.empty()) { compaction_reason_ = CompactionReason::kTtl; return; } // Periodic Compaction PickFileToCompact(vstorage_-&gt;FilesMarkedForPeriodicCompaction(), false); if (!start_level_inputs_.empty()) { compaction_reason_ = CompactionReason::kPeriodicCompaction; return; }} 首先遍历所有的level，从之前计算好的的compaction信息中得到每个level对应的score，只有当score&gt;=1才能继续进行compact的处理。 通过PickFileToCompact来选择input以及output文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273bool LevelCompactionBuilder::PickFileToCompact() { // level 0 files are overlapping. So we cannot pick more // than one concurrent compactions at this level. This // could be made better by looking at key-ranges that are // being compacted at level 0. if (start_level_ == 0 &amp;&amp; !compaction_picker_-&gt;level0_compactions_in_progress()-&gt;empty()) { TEST_SYNC_POINT(&quot;LevelCompactionPicker::PickCompactionBySize:0&quot;); return false; } start_level_inputs_.clear(); assert(start_level_ &gt;= 0); // Pick the largest file in this level that is not already // being compacted const std::vector&lt;int&gt;&amp; file_size = vstorage_-&gt;FilesByCompactionPri(start_level_); const std::vector&lt;FileMetaData*&gt;&amp; level_files = vstorage_-&gt;LevelFiles(start_level_); unsigned int cmp_idx; for (cmp_idx = vstorage_-&gt;NextCompactionIndex(start_level_); cmp_idx &lt; file_size.size(); cmp_idx++) { int index = file_size[cmp_idx]; auto* f = level_files[index]; // do not pick a file to compact if it is being compacted // from n-1 level. if (f-&gt;being_compacted) { continue; } start_level_inputs_.files.push_back(f); start_level_inputs_.level = start_level_; if (!compaction_picker_-&gt;ExpandInputsToCleanCut(cf_name_, vstorage_, &amp;start_level_inputs_) || compaction_picker_-&gt;FilesRangeOverlapWithCompaction( {start_level_inputs_}, output_level_)) { // A locked (pending compaction) input-level file was pulled in due to // user-key overlap. start_level_inputs_.clear(); continue; } // Now that input level is fully expanded, we check whether any output files // are locked due to pending compaction. // // Note we rely on ExpandInputsToCleanCut() to tell us whether any output- // level files are locked, not just the extra ones pulled in for user-key // overlap. InternalKey smallest, largest; compaction_picker_-&gt;GetRange(start_level_inputs_, &amp;smallest, &amp;largest); CompactionInputFiles output_level_inputs; output_level_inputs.level = output_level_; vstorage_-&gt;GetOverlappingInputs(output_level_, &amp;smallest, &amp;largest, &amp;output_level_inputs.files); if (!output_level_inputs.empty() &amp;&amp; !compaction_picker_-&gt;ExpandInputsToCleanCut(cf_name_, vstorage_, &amp;output_level_inputs)) { start_level_inputs_.clear(); continue; } base_index_ = index; break; } // store where to start the iteration in the next call to PickCompaction vstorage_-&gt;SetNextCompactionIndex(start_level_, cmp_idx); return start_level_inputs_.size() &gt; 0;} 首先得到当前level(start_level_)的未compacted的最大大小的文件。 通过cmp_idx索引到对应的文件。 通过ExpandInputsToCleanCut扩展当前文件的key的范围，需要满足&quot;clean cut&quot;。 通过FilesRangeOverlapWithCompaction判断是否有正在compact的out_level的文件范围和已经选择好的文件的key有overlap，如果有则跳过（clear start_level_inputs然后continue）。 最后在output_level中选择和start_level已经选择的文件的key有overlap的文件，通过ExpandInputsToCleanCut来判断output level files是否有被lock的，如果有则跳过（clear start_level_inputs然后continue）。 继续分析PickCompaction，在RocksDB中level-0比较特殊，因为只有level-0中的sst文件之间是无序的，因此接下来我们需要特殊处理level-0的情况，这个函数就是SetupOtherL0FilesIfNeeded。 1234567bool LevelCompactionBuilder::SetupOtherL0FilesIfNeeded() { if (start_level_ == 0 &amp;&amp; output_level_ != 0) { return compaction_picker_-&gt;GetOverlappingL0Files( vstorage_, &amp;start_level_inputs_, output_level_, &amp;parent_index_); } return true;} 如果调用start_level_ == 0 且 output_level_ != 0则调用GetOverlappingL0Files。 123456789101112131415161718192021222324252627bool CompactionPicker::GetOverlappingL0Files( VersionStorageInfo* vstorage, CompactionInputFiles* start_level_inputs, int output_level, int* parent_index) { // Two level 0 compaction won't run at the same time, so don't need to worry // about files on level 0 being compacted. assert(level0_compactions_in_progress()-&gt;empty()); InternalKey smallest, largest; GetRange(*start_level_inputs, &amp;smallest, &amp;largest); // Note that the next call will discard the file we placed in // c-&gt;inputs_[0] earlier and replace it with an overlapping set // which will include the picked file. start_level_inputs-&gt;files.clear(); vstorage-&gt;GetOverlappingInputs(0, &amp;smallest, &amp;largest, &amp;(start_level_inputs-&gt;files)); // If we include more L0 files in the same compaction run it can // cause the 'smallest' and 'largest' key to get extended to a // larger range. So, re-invoke GetRange to get the new key range GetRange(*start_level_inputs, &amp;smallest, &amp;largest); if (IsRangeInCompaction(vstorage, &amp;smallest, &amp;largest, output_level, parent_index)) { return false; } assert(!start_level_inputs-&gt;files.empty()); return true;} 从level-0中得到所有的重合key的文件，然后加入到start_level_inputs中。 最后调用SetupOtherInputsIfNeeded()。 123456789101112131415161718192021222324252627282930313233343536bool LevelCompactionBuilder::SetupOtherInputsIfNeeded() { // Setup input files from output level. For output to L0, we only compact // spans of files that do not interact with any pending compactions, so don't // need to consider other levels. if (output_level_ != 0) { output_level_inputs_.level = output_level_; if (!compaction_picker_-&gt;SetupOtherInputs( cf_name_, mutable_cf_options_, vstorage_, &amp;start_level_inputs_, &amp;output_level_inputs_, &amp;parent_index_, base_index_)) { return false; } compaction_inputs_.push_back(start_level_inputs_); if (!output_level_inputs_.empty()) { compaction_inputs_.push_back(output_level_inputs_); } // In some edge cases we could pick a compaction that will be compacting // a key range that overlap with another running compaction, and both // of them have the same output level. This could happen if // (1) we are running a non-exclusive manual compaction // (2) AddFile ingest a new file into the LSM tree // We need to disallow this from happening. if (compaction_picker_-&gt;FilesRangeOverlapWithCompaction(compaction_inputs_, output_level_)) { // This compaction output could potentially conflict with the output // of a currently running compaction, we cannot run it. return false; } compaction_picker_-&gt;GetGrandparents(vstorage_, start_level_inputs_, output_level_inputs_, &amp;grandparents_); } else { compaction_inputs_.push_back(start_level_inputs_); } return true;} 调用SetupOtherInputs，扩展start_level_inputs对应的output。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118// Populates the set of inputs of all other levels that overlap with the// start level.// Now we assume all levels except start level and output level are empty.// Will also attempt to expand &quot;start level&quot; if that doesn't expand// &quot;output level&quot; or cause &quot;level&quot; to include a file for compaction that has an// overlapping user-key with another file.// REQUIRES: input_level and output_level are different// REQUIRES: inputs-&gt;empty() == false// Returns false if files on parent level are currently in compaction, which// means that we can't compact thembool CompactionPicker::SetupOtherInputs( const std::string&amp; cf_name, const MutableCFOptions&amp; mutable_cf_options, VersionStorageInfo* vstorage, CompactionInputFiles* inputs, CompactionInputFiles* output_level_inputs, int* parent_index, int base_index) { assert(!inputs-&gt;empty()); assert(output_level_inputs-&gt;empty()); const int input_level = inputs-&gt;level; const int output_level = output_level_inputs-&gt;level; if (input_level == output_level) { // no possibility of conflict return true; } // For now, we only support merging two levels, start level and output level. // We need to assert other levels are empty. for (int l = input_level + 1; l &lt; output_level; l++) { assert(vstorage-&gt;NumLevelFiles(l) == 0); } InternalKey smallest, largest; // Get the range one last time. GetRange(*inputs, &amp;smallest, &amp;largest); // Populate the set of next-level files (inputs_GetOutputLevelInputs()) to // include in compaction vstorage-&gt;GetOverlappingInputs(output_level, &amp;smallest, &amp;largest, &amp;output_level_inputs-&gt;files, *parent_index, parent_index); if (AreFilesInCompaction(output_level_inputs-&gt;files)) { return false; } if (!output_level_inputs-&gt;empty()) { if (!ExpandInputsToCleanCut(cf_name, vstorage, output_level_inputs)) { return false; } } // See if we can further grow the number of inputs in &quot;level&quot; without // changing the number of &quot;level+1&quot; files we pick up. We also choose NOT // to expand if this would cause &quot;level&quot; to include some entries for some // user key, while excluding other entries for the same user key. This // can happen when one user key spans multiple files. if (!output_level_inputs-&gt;empty()) { const uint64_t limit = mutable_cf_options.max_compaction_bytes; const uint64_t output_level_inputs_size = TotalCompensatedFileSize(output_level_inputs-&gt;files); const uint64_t inputs_size = TotalCompensatedFileSize(inputs-&gt;files); bool expand_inputs = false; CompactionInputFiles expanded_inputs; expanded_inputs.level = input_level; // Get closed interval of output level InternalKey all_start, all_limit; GetRange(*inputs, *output_level_inputs, &amp;all_start, &amp;all_limit); bool try_overlapping_inputs = true; vstorage-&gt;GetOverlappingInputs(input_level, &amp;all_start, &amp;all_limit, &amp;expanded_inputs.files, base_index, nullptr); uint64_t expanded_inputs_size = TotalCompensatedFileSize(expanded_inputs.files); if (!ExpandInputsToCleanCut(cf_name, vstorage, &amp;expanded_inputs)) { try_overlapping_inputs = false; } if (try_overlapping_inputs &amp;&amp; expanded_inputs.size() &gt; inputs-&gt;size() &amp;&amp; output_level_inputs_size + expanded_inputs_size &lt; limit &amp;&amp; !AreFilesInCompaction(expanded_inputs.files)) { InternalKey new_start, new_limit; GetRange(expanded_inputs, &amp;new_start, &amp;new_limit); CompactionInputFiles expanded_output_level_inputs; expanded_output_level_inputs.level = output_level; vstorage-&gt;GetOverlappingInputs(output_level, &amp;new_start, &amp;new_limit, &amp;expanded_output_level_inputs.files, *parent_index, parent_index); assert(!expanded_output_level_inputs.empty()); if (!AreFilesInCompaction(expanded_output_level_inputs.files) &amp;&amp; ExpandInputsToCleanCut(cf_name, vstorage, &amp;expanded_output_level_inputs) &amp;&amp; expanded_output_level_inputs.size() == output_level_inputs-&gt;size()) { expand_inputs = true; } } if (!expand_inputs) { vstorage-&gt;GetCleanInputsWithinInterval(input_level, &amp;all_start, &amp;all_limit, &amp;expanded_inputs.files, base_index, nullptr); expanded_inputs_size = TotalCompensatedFileSize(expanded_inputs.files); if (expanded_inputs.size() &gt; inputs-&gt;size() &amp;&amp; output_level_inputs_size + expanded_inputs_size &lt; limit &amp;&amp; !AreFilesInCompaction(expanded_inputs.files)) { expand_inputs = true; } } if (expand_inputs) { ROCKS_LOG_INFO(ioptions_.logger, &quot;[%s] Expanding@%d %&quot; ROCKSDB_PRIszt &quot;+%&quot; ROCKSDB_PRIszt &quot;(%&quot; PRIu64 &quot;+%&quot; PRIu64 &quot; bytes) to %&quot; ROCKSDB_PRIszt &quot;+%&quot; ROCKSDB_PRIszt &quot; (%&quot; PRIu64 &quot;+%&quot; PRIu64 &quot; bytes)\\n&quot;, cf_name.c_str(), input_level, inputs-&gt;size(), output_level_inputs-&gt;size(), inputs_size, output_level_inputs_size, expanded_inputs.size(), output_level_inputs-&gt;size(), expanded_inputs_size, output_level_inputs_size); inputs-&gt;files = expanded_inputs.files; } } return true;} 将start_level_inputs和output_level_inputs加入到compaction_inputs中。 防止一些可能会出现的conflict情况，进行一些判断。 回到PickCompaction函数，最后构造一个compaction然后返回。 1234// Form a compaction object containing the files we picked.Compaction* c = GetCompaction();TEST_SYNC_POINT_CALLBACK(&quot;LevelCompactionPicker::PickCompaction:Return&quot;, c);return c; Compaction job:根据获取到数据分配compaction线程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950TEST_SYNC_POINT_CALLBACK(&quot;DBImpl::BackgroundCompaction:BeforeCompaction&quot;, c-&gt;column_family_data()); int output_level __attribute__((__unused__)); output_level = c-&gt;output_level(); TEST_SYNC_POINT_CALLBACK(&quot;DBImpl::BackgroundCompaction:NonTrivial&quot;, &amp;output_level); std::vector&lt;SequenceNumber&gt; snapshot_seqs; SequenceNumber earliest_write_conflict_snapshot; SnapshotChecker* snapshot_checker; GetSnapshotContext(job_context, &amp;snapshot_seqs, &amp;earliest_write_conflict_snapshot, &amp;snapshot_checker); assert(is_snapshot_supported_ || snapshots_.empty()); CompactionJob compaction_job( job_context-&gt;job_id, c.get(), immutable_db_options_, mutable_db_options_, file_options_for_compaction_, versions_.get(), &amp;shutting_down_, preserve_deletes_seqnum_.load(), log_buffer, directories_.GetDbDir(), GetDataDir(c-&gt;column_family_data(), c-&gt;output_path_id()), GetDataDir(c-&gt;column_family_data(), 0), stats_, &amp;mutex_, &amp;error_handler_, snapshot_seqs, earliest_write_conflict_snapshot, snapshot_checker, table_cache_, &amp;event_logger_, c-&gt;mutable_cf_options()-&gt;paranoid_file_checks, c-&gt;mutable_cf_options()-&gt;report_bg_io_stats, dbname_, &amp;compaction_job_stats, thread_pri, io_tracer_, is_manual ? &amp;manual_compaction_paused_ : nullptr, is_manual ? manual_compaction-&gt;canceled : nullptr, db_id_, db_session_id_, c-&gt;column_family_data()-&gt;GetFullHistoryTsLow(), &amp;blob_callback_); compaction_job.Prepare(); NotifyOnCompactionBegin(c-&gt;column_family_data(), c.get(), status, compaction_job_stats, job_context-&gt;job_id); mutex_.Unlock(); TEST_SYNC_POINT_CALLBACK( &quot;DBImpl::BackgroundCompaction:NonTrivial:BeforeRun&quot;, nullptr); // Should handle erorr? compaction_job.Run().PermitUncheckedError(); TEST_SYNC_POINT(&quot;DBImpl::BackgroundCompaction:NonTrivial:AfterRun&quot;); mutex_.Lock(); status = compaction_job.Install(*c-&gt;mutable_cf_options()); io_s = compaction_job.io_status(); if (status.ok()) { InstallSuperVersionAndScheduleWork(c-&gt;column_family_data(), &amp;job_context-&gt;superversion_contexts[0], *c-&gt;mutable_cf_options()); } *made_progress = true; TEST_SYNC_POINT_CALLBACK(&quot;DBImpl::BackgroundCompaction:AfterCompaction&quot;, c-&gt;column_family_data()); Prepare 1234567891011121314151617181920212223242526272829303132333435363738void CompactionJob::Prepare() { AutoThreadOperationStageUpdater stage_updater( ThreadStatus::STAGE_COMPACTION_PREPARE); // Generate file_levels_ for compaction before making Iterator auto* c = compact_-&gt;compaction; assert(c-&gt;column_family_data() != nullptr); assert(c-&gt;column_family_data()-&gt;current()-&gt;storage_info()-&gt;NumLevelFiles( compact_-&gt;compaction-&gt;level()) &gt; 0); write_hint_ = c-&gt;column_family_data()-&gt;CalculateSSTWriteHint(c-&gt;output_level()); bottommost_level_ = c-&gt;bottommost_level(); if (c-&gt;ShouldFormSubcompactions()) { { StopWatch sw(db_options_.clock, stats_, SUBCOMPACTION_SETUP_TIME); GenSubcompactionBoundaries(); } assert(sizes_.size() == boundaries_.size() + 1); for (size_t i = 0; i &lt;= boundaries_.size(); i++) { Slice* start = i == 0 ? nullptr : &amp;boundaries_[i - 1]; Slice* end = i == boundaries_.size() ? nullptr : &amp;boundaries_[i]; compact_-&gt;sub_compact_states.emplace_back(c, start, end, sizes_[i], static_cast&lt;uint32_t&gt;(i)); } RecordInHistogram(stats_, NUM_SUBCOMPACTIONS_SCHEDULED, compact_-&gt;sub_compact_states.size()); } else { constexpr Slice* start = nullptr; constexpr Slice* end = nullptr; constexpr uint64_t size = 0; compact_-&gt;sub_compact_states.emplace_back(c, start, end, size, /*sub_job_id*/ 0); }} 调用GenSubcompactionBoundaries构造subcompaction。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128void CompactionJob::GenSubcompactionBoundaries() { auto* c = compact_-&gt;compaction; auto* cfd = c-&gt;column_family_data(); const Comparator* cfd_comparator = cfd-&gt;user_comparator(); std::vector&lt;Slice&gt; bounds; int start_lvl = c-&gt;start_level(); int out_lvl = c-&gt;output_level(); // Add the starting and/or ending key of certain input files as a potential // boundary for (size_t lvl_idx = 0; lvl_idx &lt; c-&gt;num_input_levels(); lvl_idx++) { int lvl = c-&gt;level(lvl_idx); if (lvl &gt;= start_lvl &amp;&amp; lvl &lt;= out_lvl) { const LevelFilesBrief* flevel = c-&gt;input_levels(lvl_idx); size_t num_files = flevel-&gt;num_files; if (num_files == 0) { continue; } if (lvl == 0) { // For level 0 add the starting and ending key of each file since the // files may have greatly differing key ranges (not range-partitioned) for (size_t i = 0; i &lt; num_files; i++) { bounds.emplace_back(flevel-&gt;files[i].smallest_key); bounds.emplace_back(flevel-&gt;files[i].largest_key); } } else { // For all other levels add the smallest/largest key in the level to // encompass the range covered by that level bounds.emplace_back(flevel-&gt;files[0].smallest_key); bounds.emplace_back(flevel-&gt;files[num_files - 1].largest_key); if (lvl == out_lvl) { // For the last level include the starting keys of all files since // the last level is the largest and probably has the widest key // range. Since it's range partitioned, the ending key of one file // and the starting key of the next are very close (or identical). for (size_t i = 1; i &lt; num_files; i++) { bounds.emplace_back(flevel-&gt;files[i].smallest_key); } } } } } std::sort(bounds.begin(), bounds.end(), [cfd_comparator](const Slice&amp; a, const Slice&amp; b) -&gt; bool { return cfd_comparator-&gt;Compare(ExtractUserKey(a), ExtractUserKey(b)) &lt; 0; }); // Remove duplicated entries from bounds bounds.erase( std::unique(bounds.begin(), bounds.end(), [cfd_comparator](const Slice&amp; a, const Slice&amp; b) -&gt; bool { return cfd_comparator-&gt;Compare(ExtractUserKey(a), ExtractUserKey(b)) == 0; }), bounds.end()); // Combine consecutive pairs of boundaries into ranges with an approximate // size of data covered by keys in that range uint64_t sum = 0; std::vector&lt;RangeWithSize&gt; ranges; // Get input version from CompactionState since it's already referenced // earlier in SetInputVersioCompaction::SetInputVersion and will not change // when db_mutex_ is released below auto* v = compact_-&gt;compaction-&gt;input_version(); for (auto it = bounds.begin();;) { const Slice a = *it; ++it; if (it == bounds.end()) { break; } const Slice b = *it; // ApproximateSize could potentially create table reader iterator to seek // to the index block and may incur I/O cost in the process. Unlock db // mutex to reduce contention db_mutex_-&gt;Unlock(); uint64_t size = versions_-&gt;ApproximateSize(SizeApproximationOptions(), v, a, b, start_lvl, out_lvl + 1, TableReaderCaller::kCompaction); db_mutex_-&gt;Lock(); ranges.emplace_back(a, b, size); sum += size; } // Group the ranges into subcompactions const double min_file_fill_percent = 4.0 / 5; int base_level = v-&gt;storage_info()-&gt;base_level(); uint64_t max_output_files = static_cast&lt;uint64_t&gt;(std::ceil( sum / min_file_fill_percent / MaxFileSizeForLevel( *(c-&gt;mutable_cf_options()), out_lvl, c-&gt;immutable_options()-&gt;compaction_style, base_level, c-&gt;immutable_options()-&gt;level_compaction_dynamic_level_bytes))); uint64_t subcompactions = std::min({static_cast&lt;uint64_t&gt;(ranges.size()), static_cast&lt;uint64_t&gt;(c-&gt;max_subcompactions()), max_output_files}); if (subcompactions &gt; 1) { double mean = sum * 1.0 / subcompactions; // Greedily add ranges to the subcompaction until the sum of the ranges' // sizes becomes &gt;= the expected mean size of a subcompaction sum = 0; for (size_t i = 0; i + 1 &lt; ranges.size(); i++) { sum += ranges[i].size; if (subcompactions == 1) { // If there's only one left to schedule then it goes to the end so no // need to put an end boundary continue; } if (sum &gt;= mean) { boundaries_.emplace_back(ExtractUserKey(ranges[i].range.limit)); sizes_.emplace_back(sum); subcompactions--; sum = 0; } } sizes_.emplace_back(sum + ranges.back().size); } else { // Only one range so its size is the total sum of sizes computed above sizes_.emplace_back(sum); }} 遍历所有的需要compact的level,然后取得每一个level的边界(最大key和最小key)加入到bounds数组之中。 然后对获取到的bounds进行排序去重。 计算理想情况下所需要的subcompactions的个数以及输出文件的个数。 最后更新boundaries_，这里会根据文件的大小，通过平均的size,把所有的range分为几份，最终这些都会保存在boundaries_中。 Run 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185Status CompactionJob::Run() { AutoThreadOperationStageUpdater stage_updater( ThreadStatus::STAGE_COMPACTION_RUN); TEST_SYNC_POINT(&quot;CompactionJob::Run():Start&quot;); log_buffer_-&gt;FlushBufferToLog(); LogCompaction(); const size_t num_threads = compact_-&gt;sub_compact_states.size(); assert(num_threads &gt; 0); const uint64_t start_micros = db_options_.clock-&gt;NowMicros(); // Launch a thread for each of subcompactions 1...num_threads-1 std::vector&lt;port::Thread&gt; thread_pool; thread_pool.reserve(num_threads - 1); for (size_t i = 1; i &lt; compact_-&gt;sub_compact_states.size(); i++) { thread_pool.emplace_back(&amp;CompactionJob::ProcessKeyValueCompaction, this, &amp;compact_-&gt;sub_compact_states[i]); } // Always schedule the first subcompaction (whether or not there are also // others) in the current thread to be efficient with resources ProcessKeyValueCompaction(&amp;compact_-&gt;sub_compact_states[0]); // Wait for all other threads (if there are any) to finish execution for (auto&amp; thread : thread_pool) { thread.join(); } compaction_stats_.micros = db_options_.clock-&gt;NowMicros() - start_micros; compaction_stats_.cpu_micros = 0; for (size_t i = 0; i &lt; compact_-&gt;sub_compact_states.size(); i++) { compaction_stats_.cpu_micros += compact_-&gt;sub_compact_states[i].compaction_job_stats.cpu_micros; } RecordTimeToHistogram(stats_, COMPACTION_TIME, compaction_stats_.micros); RecordTimeToHistogram(stats_, COMPACTION_CPU_TIME, compaction_stats_.cpu_micros); TEST_SYNC_POINT(&quot;CompactionJob::Run:BeforeVerify&quot;); // Check if any thread encountered an error during execution Status status; IOStatus io_s; bool wrote_new_blob_files = false; for (const auto&amp; state : compact_-&gt;sub_compact_states) { if (!state.status.ok()) { status = state.status; io_s = state.io_status; break; } if (!state.blob_file_additions.empty()) { wrote_new_blob_files = true; } } if (io_status_.ok()) { io_status_ = io_s; } if (status.ok()) { constexpr IODebugContext* dbg = nullptr; if (output_directory_) { io_s = output_directory_-&gt;Fsync(IOOptions(), dbg); } if (io_s.ok() &amp;&amp; wrote_new_blob_files &amp;&amp; blob_output_directory_ &amp;&amp; blob_output_directory_ != output_directory_) { io_s = blob_output_directory_-&gt;Fsync(IOOptions(), dbg); } } if (io_status_.ok()) { io_status_ = io_s; } if (status.ok()) { status = io_s; } if (status.ok()) { thread_pool.clear(); std::vector&lt;const CompactionJob::SubcompactionState::Output*&gt; files_output; for (const auto&amp; state : compact_-&gt;sub_compact_states) { for (const auto&amp; output : state.outputs) { files_output.emplace_back(&amp;output); } } ColumnFamilyData* cfd = compact_-&gt;compaction-&gt;column_family_data(); auto prefix_extractor = compact_-&gt;compaction-&gt;mutable_cf_options()-&gt;prefix_extractor.get(); std::atomic&lt;size_t&gt; next_file_idx(0); auto verify_table = [&amp;](Status&amp; output_status) { while (true) { size_t file_idx = next_file_idx.fetch_add(1); if (file_idx &gt;= files_output.size()) { break; } // Verify that the table is usable // We set for_compaction to false and don't OptimizeForCompactionTableRead // here because this is a special case after we finish the table building // No matter whether use_direct_io_for_flush_and_compaction is true, // we will regard this verification as user reads since the goal is // to cache it here for further user reads ReadOptions read_options; InternalIterator* iter = cfd-&gt;table_cache()-&gt;NewIterator( read_options, file_options_, cfd-&gt;internal_comparator(), files_output[file_idx]-&gt;meta, /*range_del_agg=*/nullptr, prefix_extractor, /*table_reader_ptr=*/nullptr, cfd-&gt;internal_stats()-&gt;GetFileReadHist( compact_-&gt;compaction-&gt;output_level()), TableReaderCaller::kCompactionRefill, /*arena=*/nullptr, /*skip_filters=*/false, compact_-&gt;compaction-&gt;output_level(), MaxFileSizeForL0MetaPin( *compact_-&gt;compaction-&gt;mutable_cf_options()), /*smallest_compaction_key=*/nullptr, /*largest_compaction_key=*/nullptr, /*allow_unprepared_value=*/false); auto s = iter-&gt;status(); if (s.ok() &amp;&amp; paranoid_file_checks_) { OutputValidator validator(cfd-&gt;internal_comparator(), /*_enable_order_check=*/true, /*_enable_hash=*/true); for (iter-&gt;SeekToFirst(); iter-&gt;Valid(); iter-&gt;Next()) { s = validator.Add(iter-&gt;key(), iter-&gt;value()); if (!s.ok()) { break; } } if (s.ok()) { s = iter-&gt;status(); } if (s.ok() &amp;&amp; !validator.CompareValidator(files_output[file_idx]-&gt;validator)) { s = Status::Corruption(&quot;Paranoid checksums do not match&quot;); } } delete iter; if (!s.ok()) { output_status = s; break; } } }; for (size_t i = 1; i &lt; compact_-&gt;sub_compact_states.size(); i++) { thread_pool.emplace_back(verify_table, std::ref(compact_-&gt;sub_compact_states[i].status)); } verify_table(compact_-&gt;sub_compact_states[0].status); for (auto&amp; thread : thread_pool) { thread.join(); } for (const auto&amp; state : compact_-&gt;sub_compact_states) { if (!state.status.ok()) { status = state.status; break; } } } TablePropertiesCollection tp; for (const auto&amp; state : compact_-&gt;sub_compact_states) { for (const auto&amp; output : state.outputs) { auto fn = TableFileName(state.compaction-&gt;immutable_options()-&gt;cf_paths, output.meta.fd.GetNumber(), output.meta.fd.GetPathId()); tp[fn] = output.table_properties; } } compact_-&gt;compaction-&gt;SetOutputTableProperties(std::move(tp)); // Finish up all book-keeping to unify the subcompaction results AggregateStatistics(); UpdateCompactionStats(); RecordCompactionIOStats(); LogFlush(db_options_.info_log); TEST_SYNC_POINT(&quot;CompactionJob::Run():End&quot;); compact_-&gt;status = status; return status;} 遍历所有的sub_compact,然后启动线程来进行对应的compact工作，最后等到所有的线程完成，然后退出。 通过ProcessKeyValueCompaction拿到的sub_compact_states进行真正的compaction处理实际的key-value数据。 Process keys 构造能够访问所有key的迭代器 首先进入到ProcessKeyValueCompaction函数中，通过之前步骤中填充的sub_compact数据取出对应的key-value数据，构造一个InternalIterator。这一部分主要做key之间的排序以及inernal key的merge操作。 1234std::unique_ptr&lt;InternalIterator&gt; raw_input( versions_-&gt;MakeInputIterator(read_options, sub_compact-&gt;compaction, &amp;range_del_agg, file_options_for_read_));InternalIterator* input = raw_input.get(); 构造的过程是通过函数MakeInputIterator进行的，我们进入到该函数，这个函数构造迭代器的逻辑同样区分level-0和level-其他。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253InternalIterator* VersionSet::MakeInputIterator( const ReadOptions&amp; read_options, const Compaction* c, RangeDelAggregator* range_del_agg, const FileOptions&amp; file_options_compactions) { auto cfd = c-&gt;column_family_data(); // Level-0 files have to be merged together. For other levels, // we will make a concatenating iterator per level. // TODO(opt): use concatenating iterator for level-0 if there is no overlap const size_t space = (c-&gt;level() == 0 ? c-&gt;input_levels(0)-&gt;num_files + c-&gt;num_input_levels() - 1 : c-&gt;num_input_levels()); InternalIterator** list = new InternalIterator* [space]; size_t num = 0; for (size_t which = 0; which &lt; c-&gt;num_input_levels(); which++) { if (c-&gt;input_levels(which)-&gt;num_files != 0) { if (c-&gt;level(which) == 0) { const LevelFilesBrief* flevel = c-&gt;input_levels(which); for (size_t i = 0; i &lt; flevel-&gt;num_files; i++) { list[num++] = cfd-&gt;table_cache()-&gt;NewIterator( read_options, file_options_compactions, cfd-&gt;internal_comparator(), *flevel-&gt;files[i].file_metadata, range_del_agg, c-&gt;mutable_cf_options()-&gt;prefix_extractor.get(), /*table_reader_ptr=*/nullptr, /*file_read_hist=*/nullptr, TableReaderCaller::kCompaction, /*arena=*/nullptr, /*skip_filters=*/false, /*level=*/static_cast&lt;int&gt;(c-&gt;level(which)), MaxFileSizeForL0MetaPin(*c-&gt;mutable_cf_options()), /*smallest_compaction_key=*/nullptr, /*largest_compaction_key=*/nullptr, /*allow_unprepared_value=*/false); } } else { // Create concatenating iterator for the files from this level list[num++] = new LevelIterator( cfd-&gt;table_cache(), read_options, file_options_compactions, cfd-&gt;internal_comparator(), c-&gt;input_levels(which), c-&gt;mutable_cf_options()-&gt;prefix_extractor.get(), /*should_sample=*/false, /*no per level latency histogram=*/nullptr, TableReaderCaller::kCompaction, /*skip_filters=*/false, /*level=*/static_cast&lt;int&gt;(c-&gt;level(which)), range_del_agg, c-&gt;boundaries(which)); } } } assert(num &lt;= space); InternalIterator* result = NewMergingIterator(&amp;c-&gt;column_family_data()-&gt;internal_comparator(), list, static_cast&lt;int&gt;(num)); delete[] list; return result;} 首先获取当前sub_compact所属的cfd。 针对level-0,为其中的每一个sst文件构建一个table_cache迭代器，放入list中。 针对其他非level-0的层，每一层直接创建一个级联的迭代器并放入list中。也就是这个迭代器从它的start就能够顺序访问到该层最后一个sst文件的最后一个key。 将所有层的迭代器添加到一个迭代器数组list中，通过NewMergingIterator迭代器维护一个底层的排序堆结构，完成所有层之间的key-value的排序。 1234567891011121314151617InternalIterator* NewMergingIterator(const InternalKeyComparator* cmp, InternalIterator** list, int n, Arena* arena, bool prefix_seek_mode) { assert(n &gt;= 0); if (n == 0) { return NewEmptyInternalIterator&lt;Slice&gt;(arena); } else if (n == 1) { return list[0]; } else { if (arena == nullptr) { return new MergingIterator(cmp, list, n, false, prefix_seek_mode); } else { auto mem = arena-&gt;AllocateAligned(sizeof(MergingIterator)); return new (mem) MergingIterator(cmp, list, n, true, prefix_seek_mode); } }} 如果list是空的，则直接返回空。 如果只有一个，那么认为这个迭代器本身就是有序的，不需要构建一个堆排序的迭代器（level-0的sst内部是有序的，之前创建的时候是为level-0每一个sst创建一个list元素；非level-0的整层都是有序的）。 如果有多个，那么直接通过MergingIterator来创建堆排序的迭代器。 12345678910111213141516171819MergingIterator(const InternalKeyComparator* comparator, InternalIterator** children, int n, bool is_arena_mode, bool prefix_seek_mode) : is_arena_mode_(is_arena_mode), comparator_(comparator), current_(nullptr), direction_(kForward), minHeap_(comparator_), prefix_seek_mode_(prefix_seek_mode), pinned_iters_mgr_(nullptr) { children_.resize(n); for (int i = 0; i &lt; n; i++) { children_[i].Set(children[i]); } for (auto&amp; child : children_) { AddToMinHeapOrCheckStatus(&amp;child); } current_ = CurrentForward(); } 通过将传入的list也就是函数中的children中的所有元素添加到一个vector中，再遍历其中的每一个key-value，通过函数 AddToMinHeapOrCheckStatus构造底层结构堆，堆中的元素顺序是由用户参数option.comparator指定，默认是BytewiseComparator支持的lexicographical order，也就是字典顺序。 12345678void MergingIterator::AddToMinHeapOrCheckStatus(IteratorWrapper* child) { if (child-&gt;Valid()) { assert(child-&gt;status().ok()); minHeap_.push(child); } else { considerStatus(child-&gt;status()); }} 通过SeekToFirst和Next指针处理元素 回到ProcessKeyValueCompaction函数,使用构造好的internalIterator再构造一个包含所有状态的CompactionIterator，直接初始化就可以，构造完成需要将CompactionIterator的内部指针放在整个迭代器最开始的部位，通过Next指针来获取下一个key-value，同时还需要需要在每次迭代器元素内部移动的时候除了调整底层堆中的字典序结构之外，还兼顾处理各个不同type的key数据，将kValueType，kTypeDeletion，kTypeSingleDeletion，kValueDeleteRange,kTypeMerge 等不同的key type处理完成。 1234567891011c_iter-&gt;SeekToFirst();......while (status.ok() &amp;&amp; !cfd-&gt;IsDropped() &amp;&amp; c_iter-&gt;Valid()) { // Invariant: c_iter.status() is guaranteed to be OK if c_iter-&gt;Valid() // returns true. const Slice&amp; key = c_iter-&gt;key(); const Slice&amp; value = c_iter-&gt;value(); ...... c_iter-&gt;Next(); ...} Write keys 这一步也在ProcessKeyValueCompaction函数中，将key-value写入SST文件中。 确认key 的valueType类型，如果是data_block或者index_block类型，则放入builder状态机中 优先创建filter_buiilder和index_builder，index builer创建成 分层格式(两层index leve, 第一层多个restart点，用来索引具体的datablock；第二层索引第一层的index block)，方便加载到内存进行二分查找，节约内存消耗，加速查找；其次再写data_block_builder 如果key的 valueType类型是 range_deletion，则加入到range_delete_block_builder之中 先将data_block builder 利用绑定的输出的文件的writer写入底层文件 将filter_block / index_builder / compress_builder/range_del_builder/properties_builder 按照对应的格式加入到 meta_data_builder之中，利用绑定ouput 文件的 writer写入底层存储 利用meta_data_handle 和 index_handle 封装footer,写入底层存储 将builder与输出文件的writer绑定 默认的blockbase table SST文件有很多不同的block，除了data block之外，其他的block都是需要先写入到一个临时的数据结构builder，然后由builder通过其绑定的output文件的writer写入到底层磁盘形成磁盘的sst文件结构。 这里的逻辑就是将builder与output文件的writer进行绑定，创建好table builder。 1234567// Open output file if necessaryif (sub_compact-&gt;builder == nullptr) { status = OpenCompactionOutputFile(sub_compact); if (!status.ok()) { break; }} 通过table_builder的状态机添加block数据 然后调用builder-&gt;Add函数构造对应的builder结构，添加的过程主要是通过拥有三个状态的状态机完成不同block的builder创建，状态机是由构造tablebuilder的时候创建的。 1234status = sub_compact-&gt;AddToBuilder(key, value);if (!status.ok()) { break;} 1234567891011Status AddToBuilder(const Slice&amp; key, const Slice&amp; value) { auto curr = current_output(); assert(builder != nullptr); assert(curr != nullptr); Status s = curr-&gt;validator.Add(key, value); if (!s.ok()) { return s; } builder-&gt;Add(key, value); return Status::OK(); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106void BlockBasedTableBuilder::Add(const Slice&amp; key, const Slice&amp; value) { Rep* r = rep_; assert(rep_-&gt;state != Rep::State::kClosed); if (!ok()) return; ValueType value_type = ExtractValueType(key); if (IsValueType(value_type)) {#ifndef NDEBUG if (r-&gt;props.num_entries &gt; r-&gt;props.num_range_deletions) { assert(r-&gt;internal_comparator.Compare(key, Slice(r-&gt;last_key)) &gt; 0); }#endif // !NDEBUG auto should_flush = r-&gt;flush_block_policy-&gt;Update(key, value); if (should_flush) { assert(!r-&gt;data_block.empty()); r-&gt;first_key_in_next_block = &amp;key; Flush(); if (r-&gt;state == Rep::State::kBuffered) { bool exceeds_buffer_limit = (r-&gt;buffer_limit != 0 &amp;&amp; r-&gt;data_begin_offset &gt; r-&gt;buffer_limit); bool is_cache_full = false; // Increase cache reservation for the last buffered data block // only if the block is not going to be unbuffered immediately // and there exists a cache reservation manager if (!exceeds_buffer_limit &amp;&amp; r-&gt;cache_rev_mng != nullptr) { Status s = r-&gt;cache_rev_mng-&gt;UpdateCacheReservation&lt; CacheEntryRole::kCompressionDictionaryBuildingBuffer&gt;( r-&gt;data_begin_offset); is_cache_full = s.IsIncomplete(); } if (exceeds_buffer_limit || is_cache_full) { EnterUnbuffered(); } } // Add item to index block. // We do not emit the index entry for a block until we have seen the // first key for the next data block. This allows us to use shorter // keys in the index block. For example, consider a block boundary // between the keys &quot;the quick brown fox&quot; and &quot;the who&quot;. We can use // &quot;the r&quot; as the key for the index block entry since it is &gt;= all // entries in the first block and &lt; all entries in subsequent // blocks. if (ok() &amp;&amp; r-&gt;state == Rep::State::kUnbuffered) { if (r-&gt;IsParallelCompressionEnabled()) { r-&gt;pc_rep-&gt;curr_block_keys-&gt;Clear(); } else { r-&gt;index_builder-&gt;AddIndexEntry(&amp;r-&gt;last_key, &amp;key, r-&gt;pending_handle); } } } // Note: PartitionedFilterBlockBuilder requires key being added to filter // builder after being added to index builder. if (r-&gt;state == Rep::State::kUnbuffered) { if (r-&gt;IsParallelCompressionEnabled()) { r-&gt;pc_rep-&gt;curr_block_keys-&gt;PushBack(key); } else { if (r-&gt;filter_builder != nullptr) { size_t ts_sz = r-&gt;internal_comparator.user_comparator()-&gt;timestamp_size(); r-&gt;filter_builder-&gt;Add(ExtractUserKeyAndStripTimestamp(key, ts_sz)); } } } r-&gt;last_key.assign(key.data(), key.size()); r-&gt;data_block.Add(key, value); if (r-&gt;state == Rep::State::kBuffered) { // Buffered keys will be replayed from data_block_buffers during // `Finish()` once compression dictionary has been finalized. } else { if (!r-&gt;IsParallelCompressionEnabled()) { r-&gt;index_builder-&gt;OnKeyAdded(key); } } // TODO offset passed in is not accurate for parallel compression case NotifyCollectTableCollectorsOnAdd(key, value, r-&gt;get_offset(), r-&gt;table_properties_collectors, r-&gt;ioptions.logger); } else if (value_type == kTypeRangeDeletion) { r-&gt;range_del_block.Add(key, value); // TODO offset passed in is not accurate for parallel compression case NotifyCollectTableCollectorsOnAdd(key, value, r-&gt;get_offset(), r-&gt;table_properties_collectors, r-&gt;ioptions.logger); } else { assert(false); } r-&gt;props.num_entries++; r-&gt;props.raw_key_size += key.size(); r-&gt;props.raw_value_size += value.size(); if (value_type == kTypeDeletion || value_type == kTypeSingleDeletion) { r-&gt;props.num_deletions++; } else if (value_type == kTypeRangeDeletion) { r-&gt;props.num_deletions++; r-&gt;props.num_range_deletions++; } else if (value_type == kTypeMerge) { r-&gt;props.num_merge_operands++; }} kBuffered为状态机的初始状态。处于这个状态的时候，内存有较多缓存的未压缩的datablock。在该状态的过程中，通过 EnterUnbuffered 函数构造compression block，依此构建对应的index block和filterblock。最终将状态置为下一个状态的：kUnbuffered。 kUnbuffered这个状态时，compressing block已经通过之前的buffer中的data初步构造完成，且接下来将在这个状态通过 Finish 完成各个block的写入 或者通过 Abandon 丢弃当前的写入。 kClosed这个状态之前已经完成了table builder的finish或者abandon，那么接下来将析构当前的table builder。 对于第一个状态，进入下面的逻辑。如果data block能够满足flush的条件，则直接flush datablock的数据到当前bulider对应的datablock存储结构中。 123456789101112131415161718192021222324252627282930313233343536373839404142auto should_flush = r-&gt;flush_block_policy-&gt;Update(key, value); if (should_flush) { assert(!r-&gt;data_block.empty()); r-&gt;first_key_in_next_block = &amp;key; Flush(); if (r-&gt;state == Rep::State::kBuffered) { bool exceeds_buffer_limit = (r-&gt;buffer_limit != 0 &amp;&amp; r-&gt;data_begin_offset &gt; r-&gt;buffer_limit); bool is_cache_full = false; // Increase cache reservation for the last buffered data block // only if the block is not going to be unbuffered immediately // and there exists a cache reservation manager if (!exceeds_buffer_limit &amp;&amp; r-&gt;cache_rev_mng != nullptr) { Status s = r-&gt;cache_rev_mng-&gt;UpdateCacheReservation&lt; CacheEntryRole::kCompressionDictionaryBuildingBuffer&gt;( r-&gt;data_begin_offset); is_cache_full = s.IsIncomplete(); } if (exceeds_buffer_limit || is_cache_full) { EnterUnbuffered(); } } // Add item to index block. // We do not emit the index entry for a block until we have seen the // first key for the next data block. This allows us to use shorter // keys in the index block. For example, consider a block boundary // between the keys &quot;the quick brown fox&quot; and &quot;the who&quot;. We can use // &quot;the r&quot; as the key for the index block entry since it is &gt;= all // entries in the first block and &lt; all entries in subsequent // blocks. if (ok() &amp;&amp; r-&gt;state == Rep::State::kUnbuffered) { if (r-&gt;IsParallelCompressionEnabled()) { r-&gt;pc_rep-&gt;curr_block_keys-&gt;Clear(); } else { r-&gt;index_builder-&gt;AddIndexEntry(&amp;r-&gt;last_key, &amp;key, r-&gt;pending_handle); } } } EnterUnbuffered函数主要逻辑是构造compression block，如果我们开启了compression的选项则会构造。 同时依据之前flush添加到datablock中的数据来构造index block和filter block，用来索引datablock的数据。选择在这里构造的话主要还是因为flush的时候表示一个完整的datablock已经写入完成，这里需要通过一个完整的datablock数据才有必要构造一条indexblock的数据。 其中data_block_and_keys_buffers数组存放的是未经过压缩的datablock数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657for (size_t i = 0; ok() &amp;&amp; i &lt; r-&gt;data_block_buffers.size(); ++i) { if (iter == nullptr) { iter = get_iterator_for_block(i); assert(iter != nullptr); }; if (i + 1 &lt; r-&gt;data_block_buffers.size()) { next_block_iter = get_iterator_for_block(i + 1); } auto&amp; data_block = r-&gt;data_block_buffers[i]; if (r-&gt;IsParallelCompressionEnabled()) { Slice first_key_in_next_block; const Slice* first_key_in_next_block_ptr = &amp;first_key_in_next_block; if (i + 1 &lt; r-&gt;data_block_buffers.size()) { assert(next_block_iter != nullptr); first_key_in_next_block = next_block_iter-&gt;key(); } else { first_key_in_next_block_ptr = r-&gt;first_key_in_next_block; } std::vector&lt;std::string&gt; keys; for (; iter-&gt;Valid(); iter-&gt;Next()) { keys.emplace_back(iter-&gt;key().ToString()); } ParallelCompressionRep::BlockRep* block_rep = r-&gt;pc_rep-&gt;PrepareBlock( r-&gt;compression_type, first_key_in_next_block_ptr, &amp;data_block, &amp;keys); assert(block_rep != nullptr); r-&gt;pc_rep-&gt;file_size_estimator.EmitBlock(block_rep-&gt;data-&gt;size(), r-&gt;get_offset()); r-&gt;pc_rep-&gt;EmitBlock(block_rep); } else { for (; iter-&gt;Valid(); iter-&gt;Next()) { Slice key = iter-&gt;key(); if (r-&gt;filter_builder != nullptr) { size_t ts_sz = r-&gt;internal_comparator.user_comparator()-&gt;timestamp_size(); r-&gt;filter_builder-&gt;Add(ExtractUserKeyAndStripTimestamp(key, ts_sz)); } r-&gt;index_builder-&gt;OnKeyAdded(key); } WriteBlock(Slice(data_block), &amp;r-&gt;pending_handle, BlockType::kData); if (ok() &amp;&amp; i + 1 &lt; r-&gt;data_block_buffers.size()) { assert(next_block_iter != nullptr); Slice first_key_in_next_block = next_block_iter-&gt;key(); Slice* first_key_in_next_block_ptr = &amp;first_key_in_next_block; iter-&gt;SeekToLast(); std::string last_key = iter-&gt;key().ToString(); r-&gt;index_builder-&gt;AddIndexEntry(&amp;last_key, first_key_in_next_block_ptr, r-&gt;pending_handle); } } 在EnterUnbuffered函数中创建index block。 123456789101112if (table_options.index_type == BlockBasedTableOptions::kTwoLevelIndexSearch) { p_index_builder_ = PartitionedIndexBuilder::CreateIndexBuilder( &amp;internal_comparator, use_delta_encoding_for_index_values, table_options); index_builder.reset(p_index_builder_);} else { index_builder.reset(IndexBuilder::CreateIndexBuilder( table_options.index_type, &amp;internal_comparator, &amp;this-&gt;internal_prefix_transform, use_delta_encoding_for_index_values, table_options));} 回到ProcessKeyValueCompaction中的while循环中，不断遍历迭代器中的key，将其添加到对应的datablock，并完善indeblock和filter block，以及compression block。 通过构建的meta_index_builder和Footer完成数据的固化 接下来将通过FinishCompactionOutputFil对之前添加的builder数据进行整合，处理一些delete range的block以及更新当前compaction的边界。 这个函数调用是当之前累计的builder中block数据的大小达到可以写入的sst文件本身的大小max_output_file_size，会触发当前函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Close output file if it is big enough. Two possibilities determine it's // time to close it: (1) the current key should be this file's last key, (2) // the next key should not be in this file. // // TODO(aekmekji): determine if file should be closed earlier than this // during subcompactions (i.e. if output size, estimated by input size, is // going to be 1.2MB and max_output_file_size = 1MB, prefer to have 0.6MB // and 0.6MB instead of 1MB and 0.2MB) bool output_file_ended = false; if (sub_compact-&gt;compaction-&gt;output_level() != 0 &amp;&amp; sub_compact-&gt;current_output_file_size &gt;= sub_compact-&gt;compaction-&gt;max_output_file_size()) { // (1) this key terminates the file. For historical reasons, the iterator // status before advancing will be given to FinishCompactionOutputFile(). output_file_ended = true; } TEST_SYNC_POINT_CALLBACK( &quot;CompactionJob::Run():PausingManualCompaction:2&quot;, reinterpret_cast&lt;void*&gt;( const_cast&lt;std::atomic&lt;int&gt;*&gt;(manual_compaction_paused_))); if (partitioner.get()) { last_key_for_partitioner.assign(c_iter-&gt;user_key().data_, c_iter-&gt;user_key().size_); } c_iter-&gt;Next(); if (c_iter-&gt;status().IsManualCompactionPaused()) { break; } if (!output_file_ended &amp;&amp; c_iter-&gt;Valid()) { if (((partitioner.get() &amp;&amp; partitioner-&gt;ShouldPartition(PartitionerRequest( last_key_for_partitioner, c_iter-&gt;user_key(), sub_compact-&gt;current_output_file_size)) == kRequired) || (sub_compact-&gt;compaction-&gt;output_level() != 0 &amp;&amp; sub_compact-&gt;ShouldStopBefore( c_iter-&gt;key(), sub_compact-&gt;current_output_file_size))) &amp;&amp; sub_compact-&gt;builder != nullptr) { // (2) this key belongs to the next file. For historical reasons, the // iterator status after advancing will be given to // FinishCompactionOutputFile(). output_file_ended = true; } } if (output_file_ended) { const Slice* next_key = nullptr; if (c_iter-&gt;Valid()) { next_key = &amp;c_iter-&gt;key(); } CompactionIterationStats range_del_out_stats; status = FinishCompactionOutputFile(input-&gt;status(), sub_compact, &amp;range_del_agg, &amp;range_del_out_stats, next_key); RecordDroppedKeys(range_del_out_stats, &amp;sub_compact-&gt;compaction_job_stats); } FinishCompactionOutputFile函数内部最终调用s=sub_compact-&gt;builder-&gt;Finish()完成所有数据的固化写入。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Status BlockBasedTableBuilder::Finish() { Rep* r = rep_; assert(r-&gt;state != Rep::State::kClosed); bool empty_data_block = r-&gt;data_block.empty(); r-&gt;first_key_in_next_block = nullptr; Flush(); if (r-&gt;state == Rep::State::kBuffered) { EnterUnbuffered(); } if (r-&gt;IsParallelCompressionEnabled()) { StopParallelCompression();#ifndef NDEBUG for (const auto&amp; br : r-&gt;pc_rep-&gt;block_rep_buf) { assert(br.status.ok()); }#endif // !NDEBUG } else { // To make sure properties block is able to keep the accurate size of index // block, we will finish writing all index entries first. if (ok() &amp;&amp; !empty_data_block) { r-&gt;index_builder-&gt;AddIndexEntry( &amp;r-&gt;last_key, nullptr /* no next data block */, r-&gt;pending_handle); } } // Write meta blocks, metaindex block and footer in the following order. // 1. [meta block: filter] // 2. [meta block: index] // 3. [meta block: compression dictionary] // 4. [meta block: range deletion tombstone] // 5. [meta block: properties] // 6. [metaindex block] // 7. Footer BlockHandle metaindex_block_handle, index_block_handle; MetaIndexBuilder meta_index_builder; WriteFilterBlock(&amp;meta_index_builder); WriteIndexBlock(&amp;meta_index_builder, &amp;index_block_handle); WriteCompressionDictBlock(&amp;meta_index_builder); WriteRangeDelBlock(&amp;meta_index_builder); WritePropertiesBlock(&amp;meta_index_builder); if (ok()) { // flush the meta index block WriteRawBlock(meta_index_builder.Finish(), kNoCompression, &amp;metaindex_block_handle, BlockType::kMetaIndex); } if (ok()) { WriteFooter(metaindex_block_handle, index_block_handle); } r-&gt;state = Rep::State::kClosed; r-&gt;SetStatus(r-&gt;CopyIOStatus()); Status ret_status = r-&gt;CopyStatus(); assert(!ret_status.ok() || io_status().ok()); return ret_status;} Compaction参数设置 参数 说明 默认值 write_buffer_size 限定Memtable的大小 64MB level0_file_num_compaction_trigger 限定Level 0层的文件数量 4 target_file_size_base 每一层单个目标文件的大小 64MB target_file_size_multiplier 每一层单个目标文件的乘法因子 1 max_bytes_for_level_base 每一层所有文件的大小 256MB max_bytes_for_level_multiplier 每一层所有文件的乘法因子 10 level_compaction_dynamic_level_bytes 是否将Compact的策略改为层级从下往上应用 False num_levels LSM的层级数量 7 参数target_file_size_base和target_file_size_multiplier用来限定Compact之后的每一层的单个文件大小。target_file_size_base是Level-1中每个文件的大小，Level N层可以用target_file_size_base * target_file_size_multiplier ^ (L -1) 计算。target_file_size_base 默认为64MB，target_file_size_multiplier默认为1。 参数max_bytes_for_level_base和max_bytes_for_level_multiplier用来限定每一层所有文件的限定大小。 max_bytes_for_level_base是Level-1层的所有文件的限定大小。Level N层的所有文件的限定大小可以用 (max_bytes_for_level_base) * (max_bytes_for_level_multiplier ^ (L-1))计算。max_bytes_for_level_base的默认为256MB，max_bytes_for_level_multiplier默认为10。 参数level_compaction_dynamic_level_bytes用来指示Compact的策略改为层级从下往上应用。Target_Size(Ln-1) = Target_Size(Ln) / max_bytes_for_level_multiplier来限定大小：假如 max_bytes_for_level_base是 1GB, num_levels设为6。最底层的实际容量是276GB, 所以L1-L6层的大小分别是 0, 0, 0.276GB, 2.76GB, 27.6GB and 276GB。 MutableDBOptions 123456789101112131415161718192021222324252627struct MutableDBOptions { static const char* kName() { return &quot;MutableDBOptions&quot;; } MutableDBOptions(); explicit MutableDBOptions(const MutableDBOptions&amp; options) = default; explicit MutableDBOptions(const DBOptions&amp; options); void Dump(Logger* log) const; int max_background_jobs; int base_background_compactions; int max_background_compactions; uint32_t max_subcompactions; bool avoid_flush_during_shutdown; size_t writable_file_max_buffer_size; uint64_t delayed_write_rate; uint64_t max_total_wal_size; uint64_t delete_obsolete_files_period_micros; unsigned int stats_dump_period_sec; unsigned int stats_persist_period_sec; size_t stats_history_buffer_size; int max_open_files; uint64_t bytes_per_sync; uint64_t wal_bytes_per_sync; bool strict_bytes_per_sync; size_t compaction_readahead_size; int max_background_flushes;}; mutable_cf_options_ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253explicit MutableCFOptions(const ColumnFamilyOptions&amp; options) : write_buffer_size(options.write_buffer_size), max_write_buffer_number(options.max_write_buffer_number), arena_block_size(options.arena_block_size), memtable_prefix_bloom_size_ratio( options.memtable_prefix_bloom_size_ratio), memtable_whole_key_filtering(options.memtable_whole_key_filtering), memtable_huge_page_size(options.memtable_huge_page_size), max_successive_merges(options.max_successive_merges), inplace_update_num_locks(options.inplace_update_num_locks), prefix_extractor(options.prefix_extractor), disable_auto_compactions(options.disable_auto_compactions), soft_pending_compaction_bytes_limit( options.soft_pending_compaction_bytes_limit), hard_pending_compaction_bytes_limit( options.hard_pending_compaction_bytes_limit), level0_file_num_compaction_trigger( options.level0_file_num_compaction_trigger), level0_slowdown_writes_trigger(options.level0_slowdown_writes_trigger), level0_stop_writes_trigger(options.level0_stop_writes_trigger), max_compaction_bytes(options.max_compaction_bytes), target_file_size_base(options.target_file_size_base), target_file_size_multiplier(options.target_file_size_multiplier), max_bytes_for_level_base(options.max_bytes_for_level_base), max_bytes_for_level_multiplier(options.max_bytes_for_level_multiplier), ttl(options.ttl), periodic_compaction_seconds(options.periodic_compaction_seconds), max_bytes_for_level_multiplier_additional( options.max_bytes_for_level_multiplier_additional), compaction_options_fifo(options.compaction_options_fifo), compaction_options_universal(options.compaction_options_universal), enable_blob_files(options.enable_blob_files), min_blob_size(options.min_blob_size), blob_file_size(options.blob_file_size), blob_compression_type(options.blob_compression_type), enable_blob_garbage_collection(options.enable_blob_garbage_collection), blob_garbage_collection_age_cutoff( options.blob_garbage_collection_age_cutoff), max_sequential_skip_in_iterations( options.max_sequential_skip_in_iterations), check_flush_compaction_key_order( options.check_flush_compaction_key_order), paranoid_file_checks(options.paranoid_file_checks), report_bg_io_stats(options.report_bg_io_stats), compression(options.compression), bottommost_compression(options.bottommost_compression), compression_opts(options.compression_opts), bottommost_compression_opts(options.bottommost_compression_opts), bottommost_temperature(options.bottommost_temperature), sample_for_compression( options.sample_for_compression) { // TODO: is 0 fine here? RefreshDerivedOptions(options.num_levels, options.compaction_style); } Some Concepts Slice is a simple structure containing a pointer into some external storage and a size. parents &amp;&amp; grandparents: parent=level+1 grandparent==level+2 column family(cfd) compaction filter compression sst file maneger(sfm) background(bg) Reference RocksDB Compaction Wiki Rocksdb Compaction 源码详解（一）：SST文件详细格式源码解析 Rocksdb Compaction源码详解（二）：Compaction 完整实现过程 概览 Dynamic Level Size for Level-Based Compaction 通过base level减少space amplification RocksDB的Compact compaction_pri compaction filter","link":"/2021/09/24/Embedded/rocksdb_compaction/"},{"title":"Snappy Algorithm","text":"How to analyse the source codes of snappy ? In this blog, the pseudocode of compression and uncompression using snappy is given, which is aimed to help you understand snappy algorithm. Introduction Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. (For more information, see “Performance”, below.) Snappy has the following properties: Fast: Compression speeds at 250 MB/sec and beyond, with no assembler code. See “Performance” below. Stable: Over the last few years, Snappy has compressed and decompressed petabytes of data in Google’s production environment. The Snappy bitstream format is stable and will not change between versions. Robust: The Snappy decompressor is designed not to crash in the face of corrupted or malicious input. Free and open source software: Snappy is licensed under a BSD-type license. For more information, see the included COPYING file. Snappy has previously been called “Zippy” in some Google presentations and the like. Snappy in RocksDB How to link: https://github.com/facebook/rocksdb/blob/main/build_tools/build_detect_platform 123456if ! test $ROCKSDB_DISABLE_SNAPPY; then # Test whether Snappy library is installed # http://code.google.com/p/snappy/ $CXX $PLATFORM_CXXFLAGS -x c++ - -o /dev/null 2&gt;/dev/null &lt;&lt;EOF #include &lt;snappy.h&gt; int main() {} Where to use: https://github.com/facebook/rocksdb/blob/main/util/compression.h 123456789101112131415161718192021222324252627282930313233343536373839404142inline bool Snappy_Compress(const CompressionInfo&amp; /*info*/, const char* input, size_t length, ::std::string* output) {#ifdef SNAPPY output-&gt;resize(snappy::MaxCompressedLength(length)); size_t outlen; snappy::RawCompress(input, length, &amp;(*output)[0], &amp;outlen); output-&gt;resize(outlen); return true;#else (void)input; (void)length; (void)output; return false;#endif}inline CacheAllocationPtr Snappy_Uncompress( const char* input, size_t length, size_t* uncompressed_size, MemoryAllocator* allocator = nullptr) {#ifdef SNAPPY size_t uncompressed_length = 0; if (!snappy::GetUncompressedLength(input, length, &amp;uncompressed_length)) { return nullptr; } CacheAllocationPtr output = AllocateBlock(uncompressed_length, allocator); if (!snappy::RawUncompress(input, length, output.get())) { return nullptr; } *uncompressed_size = uncompressed_length; return output;#else (void)input; (void)length; (void)uncompressed_size; (void)allocator; return nullptr;#endif} RocksDB主要调用了两个接口RawCompress和RawUncompress。 Snappy Source: https://github.com/google/snappy/ 首先看一下Format，然后分别从RawCompress和RawUncompress入手分析Snappy的压缩和解压过程。 Format format_description.txt说明了一些编码格式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110Snappy compressed format descriptionLast revised: 2011-10-05This is not a formal specification, but should suffice to explain mostrelevant parts of how the Snappy format works. It is originally based ontext by Zeev Tarantov.Snappy is a LZ77-type compressor with a fixed, byte-oriented encoding.There is no entropy encoder backend nor framing layer -- the latter isassumed to be handled by other parts of the system.This document only describes the format, not how the Snappy compressor nordecompressor actually works. The correctness of the decompressor should notdepend on implementation details of the compressor, and vice versa.1. PreambleThe stream starts with the uncompressed length (up to a maximum of 2^32 - 1),stored as a little-endian varint. Varints consist of a series of bytes,where the lower 7 bits are data and the upper bit is set iff there aremore bytes to be read. In other words, an uncompressed length of 64 wouldbe stored as 0x40, and an uncompressed length of 2097150 (0x1FFFFE)would be stored as 0xFE 0xFF 0x7F.2. The compressed stream itselfThere are two types of elements in a Snappy stream: Literals andcopies (backreferences). There is no restriction on the order of elements,except that the stream naturally cannot start with a copy. (Havingtwo literals in a row is never optimal from a compression point ofview, but nevertheless fully permitted.) Each element starts with a tag byte,and the lower two bits of this tag byte signal what type of element willfollow: 00: Literal 01: Copy with 1-byte offset 10: Copy with 2-byte offset 11: Copy with 4-byte offsetThe interpretation of the upper six bits are element-dependent.2.1. Literals (00)Literals are uncompressed data stored directly in the byte stream.The literal length is stored differently depending on the lengthof the literal: - For literals up to and including 60 bytes in length, the upper six bits of the tag byte contain (len-1). The literal follows immediately thereafter in the bytestream. - For longer literals, the (len-1) value is stored after the tag byte, little-endian. The upper six bits of the tag byte describe how many bytes are used for the length; 60, 61, 62 or 63 for 1-4 bytes, respectively. The literal itself follows after the length.2.2. CopiesCopies are references back into previous decompressed data, tellingthe decompressor to reuse data it has previously decoded.They encode two values: The _offset_, saying how many bytes backfrom the current position to read, and the _length_, how many bytesto copy. Offsets of zero can be encoded, but are not legal;similarly, it is possible to encode backreferences that wouldgo past the end of the block (offset &gt; current decompressed position),which is also nonsensical and thus not allowed.As in most LZ77-based compressors, the length can be larger than the offset,yielding a form of run-length encoding (RLE). For instance,&quot;xababab&quot; could be encoded as &lt;literal: &quot;xab&quot;&gt; &lt;copy: offset=2 length=4&gt;Note that since the current Snappy compressor works in 32 kBblocks and does not do matching across blocks, it will never producea bitstream with offsets larger than about 32768. However, thedecompressor should not rely on this, as it may change in the future.There are several different kinds of copy elements, depending onthe amount of bytes to be copied (length), and how far back thedata to be copied is (offset).2.2.1. Copy with 1-byte offset (01)These elements can encode lengths between [4..11] bytes and offsetsbetween [0..2047] bytes. (len-4) occupies three bits and is storedin bits [2..4] of the tag byte. The offset occupies 11 bits, of which theupper three are stored in the upper three bits ([5..7]) of the tag byte,and the lower eight are stored in a byte following the tag byte.2.2.2. Copy with 2-byte offset (10)These elements can encode lengths between [1..64] and offsets from[0..65535]. (len-1) occupies six bits and is stored in the uppersix bits ([2..7]) of the tag byte. The offset is stored as alittle-endian 16-bit integer in the two bytes following the tag byte.2.2.3. Copy with 4-byte offset (11)These are like the copies with 2-byte offsets (see previous subsection),except that the offset is stored as a 32-bit integer instead of a16-bit integer (and thus will occupy four bytes). Compress RawCompress 123456789void RawCompress(const char* input, size_t input_length, char* compressed, size_t* compressed_length) { ByteArraySource reader(input, input_length); UncheckedByteArraySink writer(compressed); Compress(&amp;reader, &amp;writer); // Compute how many bytes were added *compressed_length = (writer.CurrentDestination() - compressed);} 首先根据参数创建reader，writer，然后调用Compress进行压缩，最后计算compressed_length。 下面看一下reader和writer的结构。 ByteArraySource 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// A Source is an interface that yields a sequence of bytesclass Source { public: Source() { } virtual ~Source(); // Return the number of bytes left to read from the source virtual size_t Available() const = 0; // Peek at the next flat region of the source. Does not reposition // the source. The returned region is empty iff Available()==0. // // Returns a pointer to the beginning of the region and store its // length in *len. // // The returned region is valid until the next call to Skip() or // until this object is destroyed, whichever occurs first. // // The returned region may be larger than Available() (for example // if this ByteSource is a view on a substring of a larger source). // The caller is responsible for ensuring that it only reads the // Available() bytes. virtual const char* Peek(size_t* len) = 0; // Skip the next n bytes. Invalidates any buffer returned by // a previous call to Peek(). // REQUIRES: Available() &gt;= n virtual void Skip(size_t n) = 0; private: // No copying Source(const Source&amp;); void operator=(const Source&amp;);};// A Source implementation that yields the contents of a flat arrayclass ByteArraySource : public Source { public: ByteArraySource(const char* p, size_t n) : ptr_(p), left_(n) { } ~ByteArraySource() override; size_t Available() const override; const char* Peek(size_t* len) override; void Skip(size_t n) override; private: const char* ptr_; size_t left_;}; Available: 表示还有多少个字节剩余。 Peek: 返回前面可以窥探到的字节流，并且返回长度。返回的buffer必须持续有效直到Skip。 Skip: 告诉Source某个部分的字节流已经不需要被使用了，将这一部分跳过。 UncheckedByteArraySink 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// A Sink is an interface that consumes a sequence of bytes.class Sink { public: Sink() { } virtual ~Sink(); // Append &quot;bytes[0,n-1]&quot; to this. virtual void Append(const char* bytes, size_t n) = 0; // Returns a writable buffer of the specified length for appending. // May return a pointer to the caller-owned scratch buffer which // must have at least the indicated length. The returned buffer is // only valid until the next operation on this Sink. // // After writing at most &quot;length&quot; bytes, call Append() with the // pointer returned from this function and the number of bytes // written. Many Append() implementations will avoid copying // bytes if this function returned an internal buffer. // // If a non-scratch buffer is returned, the caller may only pass a // prefix of it to Append(). That is, it is not correct to pass an // interior pointer of the returned array to Append(). // // The default implementation always returns the scratch buffer. virtual char* GetAppendBuffer(size_t length, char* scratch); // For higher performance, Sink implementations can provide custom // AppendAndTakeOwnership() and GetAppendBufferVariable() methods. // These methods can reduce the number of copies done during // compression/decompression. // Append &quot;bytes[0,n-1] to the sink. Takes ownership of &quot;bytes&quot; // and calls the deleter function as (*deleter)(deleter_arg, bytes, n) // to free the buffer. deleter function must be non NULL. // // The default implementation just calls Append and frees &quot;bytes&quot;. // Other implementations may avoid a copy while appending the buffer. virtual void AppendAndTakeOwnership( char* bytes, size_t n, void (*deleter)(void*, const char*, size_t), void *deleter_arg); // Returns a writable buffer for appending and writes the buffer's capacity to // *allocated_size. Guarantees *allocated_size &gt;= min_size. // May return a pointer to the caller-owned scratch buffer which must have // scratch_size &gt;= min_size. // // The returned buffer is only valid until the next operation // on this ByteSink. // // After writing at most *allocated_size bytes, call Append() with the // pointer returned from this function and the number of bytes written. // Many Append() implementations will avoid copying bytes if this function // returned an internal buffer. // // If the sink implementation allocates or reallocates an internal buffer, // it should use the desired_size_hint if appropriate. If a caller cannot // provide a reasonable guess at the desired capacity, it should set // desired_size_hint = 0. // // If a non-scratch buffer is returned, the caller may only pass // a prefix to it to Append(). That is, it is not correct to pass an // interior pointer to Append(). // // The default implementation always returns the scratch buffer. virtual char* GetAppendBufferVariable( size_t min_size, size_t desired_size_hint, char* scratch, size_t scratch_size, size_t* allocated_size); private: // No copying Sink(const Sink&amp;); void operator=(const Sink&amp;);};// A Sink implementation that writes to a flat array without any bound checks.class UncheckedByteArraySink : public Sink { public: explicit UncheckedByteArraySink(char* dest) : dest_(dest) { } ~UncheckedByteArraySink() override; void Append(const char* data, size_t n) override; char* GetAppendBuffer(size_t len, char* scratch) override; char* GetAppendBufferVariable( size_t min_size, size_t desired_size_hint, char* scratch, size_t scratch_size, size_t* allocated_size) override; void AppendAndTakeOwnership( char* bytes, size_t n, void (*deleter)(void*, const char*, size_t), void *deleter_arg) override; // Return the current output pointer so that a caller can see how // many bytes were produced. // Note: this is not a Sink method. char* CurrentDestination() const { return dest_; } private: char* dest_;}; Append: 将bytes[0,n-1]这个字节流写入。 getAppendBuffer: 交出一块length的buffer，这块length的buffer的话必须一直有效直到Append被调用。当然我们也可以直接返回scratch(外围框架分配的内存)。 Compress 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869size_t Compress(Source* reader, Sink* writer) { size_t written = 0; size_t N = reader-&gt;Available(); const size_t uncompressed_size = N; char ulength[Varint::kMax32]; char* p = Varint::Encode32(ulength, N); writer-&gt;Append(ulength, p - ulength); written += (p - ulength); internal::WorkingMemory wmem(N); while (N &gt; 0) { // Get next block to compress (without copying if possible) size_t fragment_size; const char* fragment = reader-&gt;Peek(&amp;fragment_size); assert(fragment_size != 0); // premature end of input const size_t num_to_read = std::min(N, kBlockSize); size_t bytes_read = fragment_size; size_t pending_advance = 0; if (bytes_read &gt;= num_to_read) { // Buffer returned by reader is large enough pending_advance = num_to_read; fragment_size = num_to_read; } else { char* scratch = wmem.GetScratchInput(); std::memcpy(scratch, fragment, bytes_read); reader-&gt;Skip(bytes_read); while (bytes_read &lt; num_to_read) { fragment = reader-&gt;Peek(&amp;fragment_size); size_t n = std::min&lt;size_t&gt;(fragment_size, num_to_read - bytes_read); std::memcpy(scratch + bytes_read, fragment, n); bytes_read += n; reader-&gt;Skip(n); } assert(bytes_read == num_to_read); fragment = scratch; fragment_size = num_to_read; } assert(fragment_size == num_to_read); // Get encoding table for compression int table_size; uint16_t* table = wmem.GetHashTable(num_to_read, &amp;table_size); // Compress input_fragment and append to dest const int max_output = MaxCompressedLength(num_to_read); // Need a scratch buffer for the output, in case the byte sink doesn't // have room for us directly. // Since we encode kBlockSize regions followed by a region // which is &lt;= kBlockSize in length, a previously allocated // scratch_output[] region is big enough for this iteration. char* dest = writer-&gt;GetAppendBuffer(max_output, wmem.GetScratchOutput()); char* end = internal::CompressFragment(fragment, fragment_size, dest, table, table_size); writer-&gt;Append(dest, end - dest); written += (end - dest); N -= num_to_read; reader-&gt;Skip(pending_advance); } Report(&quot;snappy_compress&quot;, written, uncompressed_size); return written;} 头部是原始串长度，使用变长整数方式Encode来编码。 123456789101112131415161718192021222324252627inline char* Varint::Encode32(char* sptr, uint32_t v) { // Operate on characters as unsigneds uint8_t* ptr = reinterpret_cast&lt;uint8_t*&gt;(sptr); static const uint8_t B = 128; if (v &lt; (1 &lt;&lt; 7)) { *(ptr++) = static_cast&lt;uint8_t&gt;(v); } else if (v &lt; (1 &lt;&lt; 14)) { *(ptr++) = static_cast&lt;uint8_t&gt;(v | B); *(ptr++) = static_cast&lt;uint8_t&gt;(v &gt;&gt; 7); } else if (v &lt; (1 &lt;&lt; 21)) { *(ptr++) = static_cast&lt;uint8_t&gt;(v | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v &gt;&gt; 7) | B); *(ptr++) = static_cast&lt;uint8_t&gt;(v &gt;&gt; 14); } else if (v &lt; (1 &lt;&lt; 28)) { *(ptr++) = static_cast&lt;uint8_t&gt;(v | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v &gt;&gt; 7) | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v &gt;&gt; 14) | B); *(ptr++) = static_cast&lt;uint8_t&gt;(v &gt;&gt; 21); } else { *(ptr++) = static_cast&lt;uint8_t&gt;(v | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v&gt;&gt;7) | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v&gt;&gt;14) | B); *(ptr++) = static_cast&lt;uint8_t&gt;((v&gt;&gt;21) | B); *(ptr++) = static_cast&lt;uint8_t&gt;(v &gt;&gt; 28); } return reinterpret_cast&lt;char*&gt;(ptr);} 获取fragment和fragmentsize。 调用CompressFragment。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180// Flat array compression that does not emit the &quot;uncompressed length&quot;// prefix. Compresses &quot;input&quot; string to the &quot;*op&quot; buffer.//// REQUIRES: &quot;input&quot; is at most &quot;kBlockSize&quot; bytes long.// REQUIRES: &quot;op&quot; points to an array of memory that is at least// &quot;MaxCompressedLength(input.size())&quot; in size.// REQUIRES: All elements in &quot;table[0..table_size-1]&quot; are initialized to zero.// REQUIRES: &quot;table_size&quot; is a power of two//// Returns an &quot;end&quot; pointer into &quot;op&quot; buffer.// &quot;end - op&quot; is the compressed size of &quot;input&quot;.namespace internal {char* CompressFragment(const char* input, size_t input_size, char* op, uint16_t* table, const int table_size) { // &quot;ip&quot; is the input pointer, and &quot;op&quot; is the output pointer. const char* ip = input; assert(input_size &lt;= kBlockSize); assert((table_size &amp; (table_size - 1)) == 0); // table must be power of two const uint32_t mask = table_size - 1; const char* ip_end = input + input_size; const char* base_ip = ip; const size_t kInputMarginBytes = 15; if (SNAPPY_PREDICT_TRUE(input_size &gt;= kInputMarginBytes)) { const char* ip_limit = input + input_size - kInputMarginBytes; for (uint32_t preload = LittleEndian::Load32(ip + 1);;) { // Bytes in [next_emit, ip) will be emitted as literal bytes. Or // [next_emit, ip_end) after the main loop. const char* next_emit = ip++; uint64_t data = LittleEndian::Load64(ip); // The body of this loop calls EmitLiteral once and then EmitCopy one or // more times. (The exception is that when we're close to exhausting // the input we goto emit_remainder.) // // In the first iteration of this loop we're just starting, so // there's nothing to copy, so calling EmitLiteral once is // necessary. And we only start a new iteration when the // current iteration has determined that a call to EmitLiteral will // precede the next call to EmitCopy (if any). // // Step 1: Scan forward in the input looking for a 4-byte-long match. // If we get close to exhausting the input then goto emit_remainder. // // Heuristic match skipping: If 32 bytes are scanned with no matches // found, start looking only at every other byte. If 32 more bytes are // scanned (or skipped), look at every third byte, etc.. When a match is // found, immediately go back to looking at every byte. This is a small // loss (~5% performance, ~0.1% density) for compressible data due to more // bookkeeping, but for non-compressible data (such as JPEG) it's a huge // win since the compressor quickly &quot;realizes&quot; the data is incompressible // and doesn't bother looking for matches everywhere. // // The &quot;skip&quot; variable keeps track of how many bytes there are since the // last match; dividing it by 32 (ie. right-shifting by five) gives the // number of bytes to move ahead for each iteration. uint32_t skip = 32; const char* candidate; if (ip_limit - ip &gt;= 16) { auto delta = ip - base_ip; for (int j = 0; j &lt; 4; ++j) { for (int k = 0; k &lt; 4; ++k) { int i = 4 * j + k; // These for-loops are meant to be unrolled. So we can freely // special case the first iteration to use the value already // loaded in preload. uint32_t dword = i == 0 ? preload : static_cast&lt;uint32_t&gt;(data); assert(dword == LittleEndian::Load32(ip + i)); uint32_t hash = HashBytes(dword, mask); candidate = base_ip + table[hash]; assert(candidate &gt;= base_ip); assert(candidate &lt; ip + i); table[hash] = delta + i; if (SNAPPY_PREDICT_FALSE(LittleEndian::Load32(candidate) == dword)) { *op = LITERAL | (i &lt;&lt; 2); UnalignedCopy128(next_emit, op + 1); ip += i; op = op + i + 2; goto emit_match; } data &gt;&gt;= 8; } data = LittleEndian::Load64(ip + 4 * j + 4); } ip += 16; skip += 16; } while (true) { assert(static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(ip)); uint32_t hash = HashBytes(data, mask); uint32_t bytes_between_hash_lookups = skip &gt;&gt; 5; skip += bytes_between_hash_lookups; const char* next_ip = ip + bytes_between_hash_lookups; if (SNAPPY_PREDICT_FALSE(next_ip &gt; ip_limit)) { ip = next_emit; goto emit_remainder; } candidate = base_ip + table[hash]; assert(candidate &gt;= base_ip); assert(candidate &lt; ip); table[hash] = ip - base_ip; if (SNAPPY_PREDICT_FALSE(static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(candidate))) { break; } data = LittleEndian::Load32(next_ip); ip = next_ip; } // Step 2: A 4-byte match has been found. We'll later see if more // than 4 bytes match. But, prior to the match, input // bytes [next_emit, ip) are unmatched. Emit them as &quot;literal bytes.&quot; assert(next_emit + 16 &lt;= ip_end); op = EmitLiteral&lt;/*allow_fast_path=*/true&gt;(op, next_emit, ip - next_emit); // Step 3: Call EmitCopy, and then see if another EmitCopy could // be our next move. Repeat until we find no match for the // input immediately after what was consumed by the last EmitCopy call. // // If we exit this loop normally then we need to call EmitLiteral next, // though we don't yet know how big the literal will be. We handle that // by proceeding to the next iteration of the main loop. We also can exit // this loop via goto if we get close to exhausting the input. emit_match: do { // We have a 4-byte match at ip, and no need to emit any // &quot;literal bytes&quot; prior to ip. const char* base = ip; std::pair&lt;size_t, bool&gt; p = FindMatchLength(candidate + 4, ip + 4, ip_end, &amp;data); size_t matched = 4 + p.first; ip += matched; size_t offset = base - candidate; assert(0 == memcmp(base, candidate, matched)); if (p.second) { op = EmitCopy&lt;/*len_less_than_12=*/true&gt;(op, offset, matched); } else { op = EmitCopy&lt;/*len_less_than_12=*/false&gt;(op, offset, matched); } if (SNAPPY_PREDICT_FALSE(ip &gt;= ip_limit)) { goto emit_remainder; } // Expect 5 bytes to match assert((data &amp; 0xFFFFFFFFFF) == (LittleEndian::Load64(ip) &amp; 0xFFFFFFFFFF)); // We are now looking for a 4-byte match again. We read // table[Hash(ip, shift)] for that. To improve compression, // we also update table[Hash(ip - 1, mask)] and table[Hash(ip, mask)]. table[HashBytes(LittleEndian::Load32(ip - 1), mask)] = ip - base_ip - 1; uint32_t hash = HashBytes(data, mask); candidate = base_ip + table[hash]; table[hash] = ip - base_ip; // Measurements on the benchmarks have shown the following probabilities // for the loop to exit (ie. avg. number of iterations is reciprocal). // BM_Flat/6 txt1 p = 0.3-0.4 // BM_Flat/7 txt2 p = 0.35 // BM_Flat/8 txt3 p = 0.3-0.4 // BM_Flat/9 txt3 p = 0.34-0.4 // BM_Flat/10 pb p = 0.4 // BM_Flat/11 gaviota p = 0.1 // BM_Flat/12 cp p = 0.5 // BM_Flat/13 c p = 0.3 } while (static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(candidate)); // Because the least significant 5 bytes matched, we can utilize data // for the next iteration. preload = data &gt;&gt; 8; } }emit_remainder: // Emit the remaining bytes as a literal if (ip &lt; ip_end) { op = EmitLiteral&lt;/*allow_fast_path=*/false&gt;(op, ip, ip_end - ip); } return op;}} // end namespace internal 核心代码是for (uint32_t preload = LittleEndian::Load32(ip + 1);;)控制的大循环。 j和k控制两层for循环，指针每次向后移动1个byte（即内层循环k每次加1，data右移8位），对于当前指针指向的4bytes内容dword，将其放入hashtable中。 如果在循环中出现了candidata==dword的情况，则将从next_emit开始的16个bytes作为literal写入op，然后goto emit_match。 1234567if (SNAPPY_PREDICT_FALSE(LittleEndian::Load32(candidate) == dword)) { *op = LITERAL | (i &lt;&lt; 2); UnalignedCopy128(next_emit, op + 1); ip += i; op = op + i + 2; goto emit_match; } 否则，进入下面的while循环。 12345678910111213141516171819202122while (true) { assert(static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(ip)); uint32_t hash = HashBytes(data, mask); uint32_t bytes_between_hash_lookups = skip &gt;&gt; 5; skip += bytes_between_hash_lookups; const char* next_ip = ip + bytes_between_hash_lookups; if (SNAPPY_PREDICT_FALSE(next_ip &gt; ip_limit)) { ip = next_emit; goto emit_remainder; } candidate = base_ip + table[hash]; assert(candidate &gt;= base_ip); assert(candidate &lt; ip); table[hash] = ip - base_ip; if (SNAPPY_PREDICT_FALSE(static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(candidate))) { break; } data = LittleEndian::Load32(next_ip); ip = next_ip; } 这就是注释中提到的启发式搜索，skip右移5位作为检查标准，不超过32bytes逐字节检查，超过32bytes不超过64bytes每两个字节检查一次…以此类推，bytes_between_hash_lookups的含义就是每多少个字节检查一次。最终会出现两种情况，一种是next_ip大于ip_limit，直接将其作为literal。另一种是data等于candidate，break跳出循环。 while循环结束后，我们得到了4bytes的match，先将match对应的literal写入op。 12assert(next_emit + 16 &lt;= ip_end);op = EmitLiteral&lt;/*allow_fast_path=*/true&gt;(op, next_emit, ip - next_emit); 然后进入emit_match这个label标记的程序段。 12345678910111213141516171819202122232425262728293031323334353637383940414243emit_match: do { // We have a 4-byte match at ip, and no need to emit any // &quot;literal bytes&quot; prior to ip. const char* base = ip; std::pair&lt;size_t, bool&gt; p = FindMatchLength(candidate + 4, ip + 4, ip_end, &amp;data); size_t matched = 4 + p.first; ip += matched; size_t offset = base - candidate; assert(0 == memcmp(base, candidate, matched)); if (p.second) { op = EmitCopy&lt;/*len_less_than_12=*/true&gt;(op, offset, matched); } else { op = EmitCopy&lt;/*len_less_than_12=*/false&gt;(op, offset, matched); } if (SNAPPY_PREDICT_FALSE(ip &gt;= ip_limit)) { goto emit_remainder; } // Expect 5 bytes to match assert((data &amp; 0xFFFFFFFFFF) == (LittleEndian::Load64(ip) &amp; 0xFFFFFFFFFF)); // We are now looking for a 4-byte match again. We read // table[Hash(ip, shift)] for that. To improve compression, // we also update table[Hash(ip - 1, mask)] and table[Hash(ip, mask)]. table[HashBytes(LittleEndian::Load32(ip - 1), mask)] = ip - base_ip - 1; uint32_t hash = HashBytes(data, mask); candidate = base_ip + table[hash]; table[hash] = ip - base_ip; // Measurements on the benchmarks have shown the following probabilities // for the loop to exit (ie. avg. number of iterations is reciprocal). // BM_Flat/6 txt1 p = 0.3-0.4 // BM_Flat/7 txt2 p = 0.35 // BM_Flat/8 txt3 p = 0.3-0.4 // BM_Flat/9 txt3 p = 0.34-0.4 // BM_Flat/10 pb p = 0.4 // BM_Flat/11 gaviota p = 0.1 // BM_Flat/12 cp p = 0.5 // BM_Flat/13 c p = 0.3 } while (static_cast&lt;uint32_t&gt;(data) == LittleEndian::Load32(candidate)); // Because the least significant 5 bytes matched, we can utilize data // for the next iteration. preload = data &gt;&gt; 8; FindMatchLength求出最大的match长度，将offset和matched写入op，最后更新hashtable。如果data和candidate不相等，退出循环。 CompressFragment结束后，回到Compress中，最后通过writer-&gt;Append(dest, end - dest)写入writer。 Uncompress RawUncompress 12345bool RawUncompress(const char* compressed, size_t compressed_length, char* uncompressed) { ByteArraySource reader(compressed, compressed_length); return RawUncompress(&amp;reader, uncompressed);} 构造ByteArraySource，将reader作为参数调用重载的RawUncompress。 1234bool RawUncompress(Source* compressed, char* uncompressed) { SnappyArrayWriter output(uncompressed); return InternalUncompress(compressed, &amp;output);} 构造SnappyArrayWriter，将output作为参数调用InternalUncompress。 12345678910template &lt;typename Writer&gt;static bool InternalUncompress(Source* r, Writer* writer) { // Read the uncompressed length from the front of the compressed input SnappyDecompressor decompressor(r); uint32_t uncompressed_len = 0; if (!decompressor.ReadUncompressedLength(&amp;uncompressed_len)) return false; return InternalUncompressAllTags(&amp;decompressor, writer, r-&gt;Available(), uncompressed_len);} 通过source构造decompressor，获取uncompressed_len，调用InternalUncompressAllTags。 12345678910111213template &lt;typename Writer&gt;static bool InternalUncompressAllTags(SnappyDecompressor* decompressor, Writer* writer, uint32_t compressed_len, uint32_t uncompressed_len) { Report(&quot;snappy_uncompress&quot;, compressed_len, uncompressed_len); writer-&gt;SetExpectedLength(uncompressed_len); // Process the entire input decompressor-&gt;DecompressAllTags(writer); writer-&gt;Flush(); return (decompressor-&gt;eof() &amp;&amp; writer-&gt;CheckLength());} 在writer中设置uncompressed_len，通过decompressor的DecompressAllTags(writer)进行解压。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120void DecompressAllTags(Writer* writer) { const char* ip = ip_; ResetLimit(ip); auto op = writer-&gt;GetOutputPtr(); // We could have put this refill fragment only at the beginning of the loop. // However, duplicating it at the end of each branch gives the compiler more // scope to optimize the &lt;ip_limit_ - ip&gt; expression based on the local // context, which overall increases speed.#define MAYBE_REFILL() \\ if (SNAPPY_PREDICT_FALSE(ip &gt;= ip_limit_min_maxtaglen_)) { \\ ip_ = ip; \\ if (SNAPPY_PREDICT_FALSE(!RefillTag())) goto exit; \\ ip = ip_; \\ ResetLimit(ip); \\ } \\ preload = static_cast&lt;uint8_t&gt;(*ip) // At the start of the for loop below the least significant byte of preload // contains the tag. uint32_t preload; MAYBE_REFILL(); for (;;) { { ptrdiff_t op_limit_min_slop; auto op_base = writer-&gt;GetBase(&amp;op_limit_min_slop); if (op_base) { auto res = DecompressBranchless(reinterpret_cast&lt;const uint8_t*&gt;(ip), reinterpret_cast&lt;const uint8_t*&gt;(ip_limit_), op - op_base, op_base, op_limit_min_slop); ip = reinterpret_cast&lt;const char*&gt;(res.first); op = op_base + res.second; MAYBE_REFILL(); } } const uint8_t c = static_cast&lt;uint8_t&gt;(preload); ip++; // Ratio of iterations that have LITERAL vs non-LITERAL for different // inputs. // // input LITERAL NON_LITERAL // ----------------------------------- // html|html4|cp 23% 77% // urls 36% 64% // jpg 47% 53% // pdf 19% 81% // txt[1-4] 25% 75% // pb 24% 76% // bin 24% 76% if (SNAPPY_PREDICT_FALSE((c &amp; 0x3) == LITERAL)) { size_t literal_length = (c &gt;&gt; 2) + 1u; if (writer-&gt;TryFastAppend(ip, ip_limit_ - ip, literal_length, &amp;op)) { assert(literal_length &lt; 61); ip += literal_length; // NOTE: There is no MAYBE_REFILL() here, as TryFastAppend() // will not return true unless there's already at least five spare // bytes in addition to the literal. preload = static_cast&lt;uint8_t&gt;(*ip); continue; } if (SNAPPY_PREDICT_FALSE(literal_length &gt;= 61)) { // Long literal. const size_t literal_length_length = literal_length - 60; literal_length = ExtractLowBytes(LittleEndian::Load32(ip), literal_length_length) + 1; ip += literal_length_length; } size_t avail = ip_limit_ - ip; while (avail &lt; literal_length) { if (!writer-&gt;Append(ip, avail, &amp;op)) goto exit; literal_length -= avail; reader_-&gt;Skip(peeked_); size_t n; ip = reader_-&gt;Peek(&amp;n); avail = n; peeked_ = avail; if (avail == 0) goto exit; ip_limit_ = ip + avail; ResetLimit(ip); } if (!writer-&gt;Append(ip, literal_length, &amp;op)) goto exit; ip += literal_length; MAYBE_REFILL(); } else { if (SNAPPY_PREDICT_FALSE((c &amp; 3) == COPY_4_BYTE_OFFSET)) { const size_t copy_offset = LittleEndian::Load32(ip); const size_t length = (c &gt;&gt; 2) + 1; ip += 4; if (!writer-&gt;AppendFromSelf(copy_offset, length, &amp;op)) goto exit; } else { const ptrdiff_t entry = kLengthMinusOffset[c]; preload = LittleEndian::Load32(ip); const uint32_t trailer = ExtractLowBytes(preload, c &amp; 3); const uint32_t length = entry &amp; 0xff; assert(length &gt; 0); // copy_offset/256 is encoded in bits 8..10. By just fetching // those bits, we get copy_offset (since the bit-field starts at // bit 8). const uint32_t copy_offset = trailer - entry + length; if (!writer-&gt;AppendFromSelf(copy_offset, length, &amp;op)) goto exit; ip += (c &amp; 3); // By using the result of the previous load we reduce the critical // dependency chain of ip to 4 cycles. preload &gt;&gt;= (c &amp; 3) * 8; if (ip &lt; ip_limit_min_maxtaglen_) continue; } MAYBE_REFILL(); } }#undef MAYBE_REFILL exit: writer-&gt;SetOutputPtr(op); } Pseudocode Performance Snappy is intended to be fast. On a single core of a Core i7 processor in 64-bit mode, it compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. (These numbers are for the slowest inputs in our benchmark suite; others are much faster.) In our tests, Snappy usually is faster than algorithms in the same class (e.g. LZO, LZF, QuickLZ, etc.) while achieving comparable compression ratios. Typical compression ratios (based on the benchmark suite) are about 1.5-1.7x for plain text, about 2-4x for HTML, and of course 1.0x for JPEGs, PNGs and other already-compressed data. Similar numbers for zlib in its fastest mode are 2.6-2.8x, 3-7x and 1.0x, respectively. More sophisticated algorithms are capable of achieving yet higher compression rates, although usually at the expense of speed. Of course, compression ratio will vary significantly with the input. Although Snappy should be fairly portable, it is primarily optimized for 64-bit x86-compatible processors, and may run slower in other environments. In particular: Snappy uses 64-bit operations in several places to process more data at once than would otherwise be possible. Snappy assumes unaligned 32 and 64-bit loads and stores are cheap. On some platforms, these must be emulated with single-byte loads and stores, which is much slower. Snappy assumes little-endian throughout, and needs to byte-swap data in several places if running on a big-endian platform. Experience has shown that even heavily tuned code can be improved. Performance optimizations, whether for 64-bit x86 or other platforms, are of course most welcome; see “Contact”, below. Reference https://github.com/google/snappy https://dirtysalt.github.io/html/snappy.html","link":"/2021/10/18/Embedded/snappy/"},{"title":"RUC, I&#39;m Coming!","text":"来咯！","link":"/2022/09/29/Life/ruc/"},{"title":"What can we learn from MIT&#39;s education?","text":"1a1d9abe5672c7bc37baaec17a6e5159f262867d797e9ec6be62be8508e8af4643c33ca0e32a5d1616085fe6db39be82dcea421b56e55196f68f3700abed5d1999ecc17075d2fbf304b6ac9ad95ad488a6657b2b8b8288651be488b85ee3947f5d558e0d5123633fc90a754b6ad99fdfb944b527cec8a02e820de70f0da60d600ac09c431d761cfe7acfd931cd569cdfbb8ea2f8e47b4f67ddb99aec5c0886a2 输入密码，查看文章","link":"/2021/04/20/Secret/mit/"},{"title":"The Adaptive Radix Tree：ARTful Indexing for Main-Memory Databases","text":"We present ART, an adaptive radix tree (trie) for efficient indexing in main memory. Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes.","link":"/2022/11/14/RUC/ICDE13-ART/"},{"title":"Fast Databases with Fast Durability and Recovery Through Multicore Parallelism","text":"Starting from an efficient multicore database system, we show that naive logging and checkpoints make normal-case execution slower, but that frequent disk synchronization allows us to keep up with many workloads with only a modest reduction in throughput. We design throughout for parallelism: during logging, during checkpointing, and during recovery. The result is fast.","link":"/2022/12/07/RUC/OSDI14-zheng/"},{"title":"ffmpeg","text":"ffmpeg常用命令。 常用命令 查看媒体文件详细信息 $ ffmpeg -i video.mp4 转换视频格式flv-&gt;mp4 $ ffmpeg -i input.flv output.mp4 从一个媒体文件移除视频流 $ ffmpeg -i input.mp4 -vn output.mp3 从一个视频文件移除音频流 $ ffmpeg -i input.mp4 -an output.mp4 预览或测试视频或音频文件 $ ffplay video.mp4 $ ffplay audio.mp3 增加视频播放速度 $ ffmpeg -i input.mp4 -vf &quot;setpts=0.5*PTS&quot; output.mp4 减少视频播放速度 $ ffmpeg -i input.mp4 -vf &quot;setpts=4.0*PTS&quot; output.mp4 获取帮助 $ man ffmpeg Reference https://zhuanlan.zhihu.com/p/67878761","link":"/2021/09/27/Tools/ffmpeg/"},{"title":"The Use of &quot;(void)val&quot;","text":"Have you ever seen “(void)val” in codes ? Why (void)val 作用是避免编译器警告。如果声明/定义了但未使用的变量，在编译时会生成warning。如果项目里打开了-Werror选项，会将warning视为error，这样的话无法通过编译，所以需要用这种方法绕过无关紧要的warning。","link":"/2021/10/11/Tricks/void/"}],"tags":[{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"distributed","slug":"distributed","link":"/tags/distributed/"},{"name":"RocksDB","slug":"RocksDB","link":"/tags/RocksDB/"},{"name":"db_bench","slug":"db-bench","link":"/tags/db-bench/"},{"name":"cpp","slug":"cpp","link":"/tags/cpp/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"buffer pool","slug":"buffer-pool","link":"/tags/buffer-pool/"},{"name":"compaction","slug":"compaction","link":"/tags/compaction/"},{"name":"compression","slug":"compression","link":"/tags/compression/"},{"name":"personal","slug":"personal","link":"/tags/personal/"},{"name":"ffmpeg","slug":"ffmpeg","link":"/tags/ffmpeg/"}],"categories":[{"name":"Embedded","slug":"Embedded","link":"/categories/Embedded/"},{"name":"Life","slug":"Life","link":"/categories/Life/"},{"name":"Secret","slug":"Secret","link":"/categories/Secret/"},{"name":"RUC","slug":"RUC","link":"/categories/RUC/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"Tricks","slug":"Tricks","link":"/categories/Tricks/"}]}